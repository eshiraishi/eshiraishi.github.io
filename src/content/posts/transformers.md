---
title: üáßüá∑ Explicando Transformers
description: "Aten√ß√£o √© realmente tudo que voc√™ precisa?"
date: '2025-01-31'
---

Em 2017, a Google Brain lan√ßou o artigo "Attention is All you Need", que introduziu para o mundo o Transformer, uma arquitetura para redes neurais para transdu√ß√£o de sequ√™ncias baseada em aten√ß√£o que permitiu a cria√ß√£o de modelos que superaram todos os outros modelos anteriores em tradu√ß√£o entre idiomas.

Anos depois, essa arquitetura deu origem a muitas alternativas que atingiram o Estado da Arte m√∫ltiplas em outras aplica√ß√µes, em especial na cria√ß√£o de modelos de linguagem, que popularizaram fortemente o uso de IA Generativa para muitas aplica√ß√µes de processamento de linguagem natural. No momento de escrita desse post, s√£o varia√ß√µes do Transformer como os Generative Pretrained Transformers (GPTs) que est√£o sendo usadas por tr√°s de intelig√™ncias artificiais avan√ßadas, como ChatGPT, Claude e Gemini, mostrando o potencial dessa arquitetura e o seu legado na hist√≥ria da intelig√™ncia artificial.

Para ajudar a entender o que os Transformers t√™m de especial, esse post vai explicar como funcionam os Transformers, atrav√©s de uma implementa√ß√£o de exemplo constru√≠da do zero em Python usando PyTorch, e assumindo poucos pr√©-requisitos.

Embora j√° existam muitas implementa√ß√µes de Transformers dispon√≠veis na internet, durante o momento de escrita desse post, n√£o existiam muitos artigos explicando o t√≥pico em Portugu√™s de forma did√°tica, o que motivou a escrita.

Infelizmente, se absolutamente nenhum pr√©-requisito for assumido, o conte√∫do ficar√° extenso demais para ser feito de uma vez (embora talvez seja poss√≠vel escrever outros posts com os pr√©-requisitos no futuro). Por isso, para entender tudo que ser√° explicado, √© importante entender os conte√∫dos a seguir:

* Como funcionam opera√ß√µes matriciais como produto interno, produto matricial, transposi√ß√£o de matrizes, etc.
* Como funcionam os componentes de redes neurais feed-forward
* Como programar em Python ao n√≠vel de criar classes e objetos e interagir com bibliotecas terceiras
* Como usar os componentes b√°sicos do PyTorch, como tensores, dispositivos, m√≥dulos, autograd, otimizadores, etc.
* Porque realizar opera√ß√µes em lote usando opera√ß√µes matriciais podem ser muito mais r√°pido que realiz√°-las individualmente.

Al√©m disso, a se√ß√£o de recursos adicionais possui alguns conte√∫dos em Ingl√™s que explicam esses mencionados.

## Objetivo

De forma geral, Transformers podem ser usados para modelar aplica√ß√µes de transdu√ß√£o de sequ√™ncias, que s√£o aplica√ß√µes onde √© necess√°rio gerar uma sequ√™ncia de elementos a partir de outra. Isso faz com que muitas aplica√ß√µes possam ser modeladas como uma transdu√ß√£o de sequ√™ncias, como tradu√ß√£o autom√°tica, gera√ß√£o textual, sumariza√ß√£o de textos e s√≠ntese de mol√©culas.

De forma mais completa, uma fun√ß√£o de transdu√ß√£o de sequ√™ncias $M(s)$ relaciona uma sequ√™ncia ordenada $s = <s_1,s_2,\cdots,s_{n}>$, composta de elementos do conjunto enumer√°vel $S$ a outra sequ√™ncia $t = <t_1,t_2,\cdots,t_{m}>$, composta de elementos do conjunto enumer√°vel $T$.

Para facilitar a explica√ß√£o sobre o funcionamento dos Transformers, al√©m da nota√ß√£o formal, ser√£o usados exemplos de uma aplica√ß√£o espec√≠fica. Dada a import√¢ncia dos Transformers para o progresso de intelig√™ncias artificiais como os Large Language Models, os exemplos desse post ser√£o baseados no contexto da cria√ß√£o de um modelo de linguagem.

De forma geral, a ideia de um modelo de linguagem √© determinar as pr√≥ximas letras de um texto a partir dos seus elementos anteriores, assim como nos corretores autom√°ticos usados nos teclados digitais de smartphones, por exemplo.

Adaptando a nota√ß√£o anterior, um modelo de linguagem $M(s)$ relaciona um texto $s = <s_1,s_2,\cdots,s_{n}>$, composto de letras do alfabeto $S$ a outro texto $s' = <s'_1,s'_2,\cdots,s'_{m}>$, tamb√©m composto de letras do alfabeto $S$. Note que $S$ e $T$ serem iguais √© uma especificidade de aplica√ß√µes como modelos de linguagem. Em tradu√ß√£o autom√°tica, por exemplo, os textos podem estar em idiomas diferentes (logo os alfabetos podem ser diferentes).

### Observa√ß√£o

Al√©m de criar um modelo de linguagem de forma geral, um grande desafio na cria√ß√£o de redes neurais √© minimizar o tempo de infer√™ncia desses modelos. Embora seja poss√≠vel implementar um Transformer usando apenas estruturas como vari√°veis, listas e la√ßos de repeti√ß√£o, esse tipo de implementa√ß√£o se torna lento demais para treinar e usar na pr√°tica pelo grande n√∫mero de computa√ß√µes que precisam ser realizadas de forma ineficiente. Portanto, √© necess√°rio considerar t√©cnicas de programa√ß√£o paralela desde o in√≠cio da implementa√ß√£o do modelo.

Em geral, a forma mais pr√°tica de implementar paralelismo em redes neurais √© aproveitar a capacidade dos dispositivos modernos de realizar opera√ß√µes matem√°ticas sobre vetores, matrizes e tensores em paralelo de forma muito eficiente. Portanto, mudando a representa√ß√£o dos dados durante a execu√ß√£o das opera√ß√µes, √© poss√≠vel gerar algoritmos eficientes de forma simples, embora isso exija que os algoritmos sejam definidos levando em considera√ß√£o dados multidimensionais desde a sua concep√ß√£o.

### Um pouco de hist√≥ria

Criar modelos eficientes para transdu√ß√£o de sequ√™ncias foi um problema em aberto por muitos anos para a comunidade cient√≠fica, onde o desafio era resolver problemas comuns ao treino de arquiteturas alternativas aos Transformers.

T√©cnicas baseadas em redes neurais recorrentes possuiam a melhor performance em aplica√ß√µes como tradu√ß√£o textual, como foi o caso da arquitetura proposta pela Google para o Google Tradutor em 2014 no influente artigo "Sequence to Sequence Learning with Neural Networks"). Embora explicar esse tipo de arquitetura esteja fora do escopo desse post, existiam problemas comuns ao processo de treinamento de redes neurais recorrentes que encorajaram o desenvolvimento de alternativas:

1. A sua natureza recursiva pode causar problemas de gradient vanishing, efeito que ocorre quando os valores dos gradientes gerados via backpropagation s√£o pequenos demais para que o modelo consiga convergir para o m√≠nimo global at√© o final do treinamento, praticamente parando antes do valor da fun√ß√£o de perda se tornar pr√≥ximo do m√≠nimo global.

2. A ordem sequencial das opera√ß√µes envolvidas nesse tipo de modelo pode tornar a infer√™ncia muito lentas, o que pode tornar o treinamento e uso posterior do modelo invi√°vel.

3. A incapacidade do modelo de entender o significado de uma palavra em uma frase com base no seu contexto pode comprometer o texto gerado.

Esses problemas eram cr√≠ticos para o uso desses modelos em muitas aplica√ß√µes, o que incentivou a pesquisa sobre a cria√ß√£o de arquiteturas alternativos que convergissem mais rapidamente e em modelos com melhor performance. Em especial, a cria√ß√£o de modelos capazes de resignificar palavras com base no seu contexto para resolver parte desses problemas levou ao estudo de mecanismos de aten√ß√£o, que t√™m justamente esse objetivo.

Assim como grande parte da pesquisa em redes neurais, muitas das decis√µes e conven√ß√µes que ser√£o usadas s√£o baseadas no que levou aos melhores resultados nos experimentos conduzidos. Por√©m, no caso dos Transformers, tamb√©m existem escolhas tomadas com o objetivo de garantir que os algoritmos s√£o eficientes computacionalmente em treino e infer√™ncia e n√£o sofram dos mesmos problemas de converg√™ncia que as alternativas.

Nesse sentido, o grande diferencial dos Transformers √© o fato de serem baseados apenas em redes neurais feed-forward e mecanismos de aten√ß√£o, o que leva a modelos capazes de resolver os problemas mencionados anteriormente em diversas aplica√ß√µes e atingir a melhor performance at√© ent√£o em transdu√ß√£o de sequ√™ncias no processo.

Essa conclus√£o explica o nome do artigo: do ponto de vista da arquitetura do modelo, redes neurais recorrentes n√£o s√£o necess√°rias para criar modelos eficientes, apenas mecanismos de aten√ß√£o, ou seja, "Attention is All you Need".

## Representa√ß√£o dos dados

Para conseguirmos trabalhar com textos, √© necess√°rio definir uma representa√ß√£o num√©rica equivalente a um texto para que essa representa√ß√£o possa ser usada pelos Transformers. A abordagem que a maioria das t√©cnicas usa para realizar esse processo √© converter esses valores em tokens e embeddings.

### Tokens

Como textos s√£o representados por um alfabeto finito e conhecido, √© poss√≠vel enumerar todos os caracteres desse alfabeto e criar uma fun√ß√£o que os associa a uma representa√ß√£o num√©rica √∫nica. Essas representa√ß√µes num√©ricas s√£o conhecidas como tokens, e a fun√ß√£o como tokenizer.

Al√©m de enumerar caracteres individuais, √© poss√≠vel enumerar sequ√™ncias de caracteres, formando palavras ou n-gramas (sequ√™ncias ordenadas de n caracteres), gerando tokenizers com ainda mais tokens. Essa √© uma pr√°tica realizada muito frequentemente em modelos de linguagem, com o objetivo de melhorar a performance desses modelos de forma geral. usando algortimos como Byte Pair Encoding, WordPiece e SentencePiece, √© poss√≠vel criar tokenizers com centenas de milhares de tokens.

Por√©m, como o modelo de linguagem assume que o texto j√° passou pelo tokenizer previamente, a escolha do m√©todo de tokeniza√ß√£o n√£o afeta a arquitetura de nenhuma forma. Por isso, para simplificar a explica√ß√£o, usaremos um tokenizer simples composto apenas pelos caracteres imprim√≠veis da tabela ASCII, dispon√≠vel no objeto `printable` do m√≥dulo `string` em Python.

| Letra   | Token   |
|---------|---------|
| `a`     | `1`     |
| `b`     | `2`     |
| `c`     | `3`     |
| ...     | ...     |
| `z`     | `26`    |
| `‚ê£`     | `27`    |
| `.`     | `28`    |
| `,`     | `29`    |

Uma sequ√™ncia de tokens gerada a partir de um texto pode ser representanda usando um vetor com os valores de cada token em ordem. Concatenando esses vetores como linhas, √© poss√≠vel representar um batch de textos usando uma matriz, que ser√° a dimens√£o esperada pelo modelo para maximizar a sua efici√™ncia computacional.

$$
\begin{array}{cccc}
    \texttt{g} & \texttt{a} & \texttt{t} & \texttt{o}
\end{array} \\
\downarrow \\
\begin{bmatrix}
    7  &  1  & 20 & 15
\end{bmatrix}
$$

Al√©m disso, o tokenizer adiciona dois tokens especiais √† sequ√™ncia. O primeiro token, `<bos>` (beginning of sentence), representa o in√≠cio do texto, e o segundo token, `<eos>` (end of sentence), representa o final do texto.

Nos exemplos, `<bos>` e `<eos>` sempre ser√£o representados numericamente por `1` e `2`, respectivamente, logo, as representa√ß√µes num√©ricas mencionadas anteriormente come√ßam a partir de `3`.

| Letra       | Token   |
|-------------|---------|
| `<bos>`     | `1`     |
| `<eos>`     | `2`     |
| `a`         | `3`     |
| `b`         | `4`     |
| `c`         | `5`     |
| ...         | ...     |
| `z`         | `28`    |
| `‚ê£`         | `29`    |
| `.`         | `30`    |
| `,`         | `31`    |

$$
\begin{array}{cccc}
    \texttt{g} & \texttt{a} & \texttt{t} & \texttt{o}
\end{array} \\
\downarrow \\
\begin{array}{cccccc}
    \texttt{<bos>} & \texttt{g} & \texttt{a} & \texttt{t} & \texttt{o}  & \texttt{<eos>}
\end{array} \\
\downarrow \\
\begin{bmatrix}
    1 & 9  &  3  & 22 & 17 & 2
\end{bmatrix}
$$

Os tokens `<bos>` e `<eos>` tamb√©m podem ser usados para separar a sequ√™ncia recebida da sequ√™ncia gerada, o que ser√° √∫til no treinamento dos Transformers futuramente. Por exemplo, se o modelo recebeu o texto "cachorro" e gerou o texto "dog", essas sequ√™ncias pode ser descritas como sendo uma sequ√™ncia s√≥:

$$
\begin{array}{cccccc}
\texttt{<bos>} & \texttt{c} & \texttt{a} & \texttt{c} & \texttt{h} & \texttt{o} & \texttt{r} & \texttt{r} & \texttt{o} & \texttt{<eos>} & \texttt{<bos>} & \texttt{d} & \texttt{o} & \texttt{g} & \texttt{<eos>}
\end{array}
$$

```python
from string import printable

char_ids = {
    '<bos>': 1,
    '<eos>': 2,
}

char_ids.update({
    char: index + 2
    for index, char in enumerate(printable, 1)
})


def tokenize(chars: str, char_ids: dict[str,int]) -> list[int]:
    tokens = [1]
    tokens.extend(
        char_ids[char]
        for char in chars
    )
    return [*tokens, 2]
```

### Padding

Como introduzido anteriormente, uma forma de desenvolver modelos r√°pidos de forma simples √© maximizar o n√∫mero de opera√ß√µes realizadas em batch usando opera√ß√µes tensoriais.

Um texto ap√≥s ser transformado em sequ√™ncia de tokens pode ser representado como um vetor, e ao realizar essa transforma√ß√£o em todos os textos de um lote e concaten√°-los, √© poss√≠vel criar uma matriz, que representa um lote de textos e permite a acelera√ß√£o de opera√ß√µes tensoriais nas pr√≥ximas etapas. Por√©m, concatenar esses vetores para criar uma matriz assume que todos os elementos do lote possuem o mesmo comprimento, mas como os textos originais podem ter comprimentos diferentes, essa premissa n√£o √© sempre v√°lida.

Uma forma de garantir que todos os vetores de um batch sempre ter√£o o mesmo comprimento √© realizar padding nos vetores, processo onde tokens especiais s√£o adicionados repetidamente a cada sequ√™ncia at√© que todas tenham $n$ elementos.

Esse token especial √© denominado padding token, representado por `<pad>` e nos exemplos, sempre ser√° representado numericamente por `0`. Tamb√©m assuma que todos um batch sempre tem $b$ elementos.

| Letra   | Token   |
|---------|---------|
| `<pad>` | `0`     |
| `<bos>` | `1`     |
| `<eos>` | `2`     |
| `a`     | `4`     |
| `b`     | `5`     |
| `c`     | `6`     |
| ...     | ...     |
| `z`     | `28`    |
| `‚ê£`     | `29`    |
| `.`     | `30`    |
| `,`     | `31`    |

Por fim, existem algumas formas de determinar $t$, o n√∫mero de elementos que cada sequ√™ncia ter√° ap√≥s o padding:

1. Definir $t$ como sendo o comprimento da maior sequ√™ncia do batch.
2. Definir $t$ como sendo um valor constante e arbitr√°rio.

Por exemplo, um lote de textos transformado em tokens e usando padding seguindo a op√ß√£o 1 seria feito da seguinte forma:

$$
\begin{array}{c}
    \text{gato} \\
    \text{elefante} \\
    \text{peixe} \\
    \text{p√°ssaro} \\
    \text{c√£o}
\begin{array}{c}
$$

$$
\downarrow
$$

$$
\begin{array}{cccccccc}
    \texttt{<bos>} & \texttt{ g } & \texttt{ a } & \texttt{ t } & \texttt{ o } & \texttt{<eos>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>} \\
    \texttt{<bos>} & \texttt{e} & \texttt{l} & \texttt{e} & \texttt{f} & \texttt{a} & \texttt{n} & \texttt{t} & \texttt{e}  & \texttt{<eos>} \\
    \texttt{<bos>} & \texttt{p} & \texttt{e} & \texttt{i} & \texttt{x} & \texttt{e}  & \texttt{<eos>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>} \\
    \texttt{<bos>} & \texttt{p} & \texttt{√°} & \texttt{s} & \texttt{s} & \texttt{a} & \texttt{r} & \texttt{o}  & \texttt{<eos>} & \texttt{<pad>} \\
    \texttt{<bos>} & \texttt{c} & \texttt{√£} & \texttt{o} & \texttt{<eos>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>} & \texttt{<pad>}
\end{array}
$$

$$
\downarrow
$$

$$
\begin{bmatrix}
   1 &  7  &  1  & 20 & 15 &  2 &  0 &  0 &  0 &  0 \\
   1 &  5  & 12  &  5 &  6 &  1 & 14 & 20 &  5  &  2\\
   1 & 16  &  5  &  9 & 24 &  5  &  2 &  0 &  0 &  0 \\
   1 & 16  & 27  & 19 & 19 &  1 & 18 & 15  &  2 &  0 \\
   1 &  3  & 28  & 15  &  2 &  0 &  0 &  0 &  0 &  0
\end{bmatrix}
$$

De forma geral, o aumento de velocidade obtido pelo padding √© grande o suficiente para justificar o uso adicional de mem√≥ria, j√° que o tempo gasto √© frequentemente um limitante maior que a mem√≥ria gasta por tipo de rede neural durante o treinamento e infer√™ncia.

As duas alternativas podem impactar a velocidade das opera√ß√µes dependendo do tipo de hardware usado. Em CPUs e GPUs, a op√ß√£o 1 pode ser mais vantajosa por economizar mem√≥ria, j√° que nesse caso a op√ß√£o 2 n√£o gera nenhum aumento de velocidade. J√° em TPUs, opera√ß√µes sobre batches de tamanhos fixos podem ser mais r√°pidas que as mesmas opera√ß√µes em batches de tamanhos diferentes, logo a op√ß√£o 2 pode ser mais vantajosa.

```python
char_ids = {
    '<pad>': 0,
    '<bos>': 1,
    '<eos>': 2,
}

char_ids.update({
    char: index + 3
    for index, char in enumerate(printable)
})


def tokenize(chars: str, char_ids: dict[str, int]) -> list[int]:
    tokens = [1]
    tokens.extend(
        char_ids[char]
        for char in chars
    )
    return [*tokens, 2]

def tokenize_and_pad(inputs: list[str]) -> torch.Tensor:
    chars, *inputs = inputs

    tokens = tokenize(chars, char_ids)
    token_lists = [tokens]

    token_count = len(tokens)
    token_counts = [token_count]

    max_length = token_count

    for chars in inputs:
        tokens = tokenize(chars)
        token_lists.append(tokens)

        token_count = len(tokens)
        token_counts.append(token_count)

        max_length = max(max_length, token_count)

    for tokens, token_count in zip(token_lists, token_counts):
        tokens.extend(
            0
            for _ in range(max_length - token_count)
        )

    tokens_tensor = torch.tensor(token_lists)
    return tokens_tensor, token_counts
```

Por curiosidade, o m√≥dulo `torch.nested` permite representar matrizes a partir de uma lista de vetores de comprimentos variados. Por√©m, esse m√≥dulo ainda est√° em fase experimental, e usar esse recurso pode deixar as opera√ß√µes mais lentas que as outras alternativas.

### Trunca√ß√£o

Mesmo ao escolher a op√ß√£o 1, tamb√©m √© comum definir um tamanho m√°ximo que as sequ√™ncias podem ter, e caso uma sequ√™ncia tenha originalmente mais tokens que esse limite, ela ser√° truncada, fazendo com seus √∫ltimos elementos sejam descartados.

Em ambos os casos, o valor de $t$ geralmente √© escolhido com base na mem√≥ria dispon√≠vel ou determinando empiricamente o valor para o comprimento de uma sequ√™ncia onde, em m√©dia, os modelos sendo treinados n√£o conseguem considerar toda a sequ√™ncia recebida durante a gera√ß√£o.

### Embeddings

A representa√ß√£o num√©rica dos tokens √© uma forma simples de converter caracteres para valores num√©ricos. Por√©m, us√°-la diretamente como espa√ßo de representa√ß√£o dos elementos da sequ√™ncia recebida durante o treinamento de modelos pode criar vieses indesejados durante o treinamento.

Esse vi√©s ocorre porque usar a representa√ß√£o num√©rica literal far√° com que o modelo trate elementos com o token $n$ como sendo valores menores que elementos com o token $n+1$, j√° que ambos s√£o inteiros. Embora essa rela√ß√£o n√£o seja verdadeira, √© poss√≠vel que o modelo encontre padr√µes que n√£o existem nos conjunto de dados de treinamento, dificultando o processo. Ao transformar esses valores em vetores de dimens√£o $t$, se o espa√ßo for adequado, essa rela√ß√£o de ordem n√£o estar√° presente.

Embeddings s√£o representa√ß√µes do espa√ßo de uma vari√°vel (por exemplo, os inteiros para a representa√ß√£o num√©rica dos tokens) em outro espa√ßo. Em aprendizado de m√°quina, esse espa√ßo geralmente possui $d$ dimens√µes, onde $d$ √© um hiperpar√¢metro diferente das dimens√µes do espa√ßo original e tanto $d$ quanto o espa√ßo s√£o escolhidos com o objetivo de representar os valores originais de outra forma que auxilie o treinamento de modelos.

Independentemente da t√©cnica usada para determinar os embeddings para cada elemento, o batch de sequ√™ncias recebidas ser√° transformado de uma matriz de dimens√£o $b \times t$ para cada um tensor de dimens√£o $b \times t \times d$.

$$
    \begin{bmatrix}
        1   & 14    & \dots  & 10 & 2 \\
        1   & 25    & \dots  & 0 & 2 \\
        \vdots & \vdots  & \ddots & \vdots & \vdots \\
        1   & 5    & \dots & 0 & 2
    \end{bmatrix}
$$

$$
    \downarrow
$$

$$
    \begin{bmatrix}
        \begin{bmatrix}
            0.23   & 1.45    & \dots  & 2.67 \\
            3.14   & 4.56    & \dots  & 5.78 \\
            \vdots & \vdots  & \ddots & \vdots \\
            1.76   & 0.65    & \dots  & 0.13 \\
            6.89 & 7.01 & \dots & 8.23 \\
        \end{bmatrix} \\
        \quad \\
        \begin{bmatrix}
            0.23   & 1.45    & \dots  & 2.67 \\
            9.34   & 0.12    & \dots  & 1.34 \\
            \vdots & \vdots  & \ddots & \vdots \\
            1.49   & 5.59    & \dots  & 0.33 \\
            6.89   & 7.01    & \dots  & 8.23 \\
        \end{bmatrix} \\
        \vdots \\
        \begin{bmatrix}
            0.23   & 1.45    & \dots  & 2.67 \\
            5.79 & 6.80 & \dots & 7.91 \\
            \vdots & \vdots  & \ddots & \vdots \\
            1.49   & 5.59    & \dots  & 0.33 \\
            6.89 & 7.01 & \dots & 8.23 \\
        \end{bmatrix}
    \end{bmatrix}
$$

Usando uma das formas mais simples de se transformar tokens em embeddings de forma eficiente, primeiro √© necess√°rio aplicar one-hot encoding sobre cada elemento, transformando-os vetores esparsos (que possui grande dimensionalidade mas muitos valores nulos).

Se $|T|$ √© o tamanho do vocabul√°rio, a representa√ß√£o do texto "baba" usando One-Hot Encoding ser√°:

$$
    \begin{array}{cccccccccc}
        \texttt{<bos>} & \texttt{b} & \texttt{a} & \texttt{b} & \texttt{a} & \texttt{<eos>}
    \end{array}
$$

$$
    \downarrow
$$

$$
    \begin{bmatrix}
        1 & 4 & 3 & 4 & 3 & 2 \\
    \end{bmatrix}
$$

$$
    \downarrow
$$

$$
\begin{array}{c|cccccc}
    & 0 & 1 & 2 & 3 & 4 & \dots & |T| \\
    \hline
    1 & 0 & 1 & 0 & 0 & 0 & \dots & 0 \\
    4 & 0 & 0 & 0 & 0 & 1 & \dots & 0 \\
    3 & 0 & 0 & 0 & 1 & 0 & \dots & 0 \\
    4 & 0 & 0 & 0 & 0 & 1 & \dots & 0 \\
    3 & 0 & 0 & 0 & 1 & 0 & \dots & 0 \\
    2 & 0 & 0 & 1 & 0 & 0 & \dots & 0
\end{array}
$$

One-Hot Encoding √© usado por permitir acessar elementos individuais de uma matriz a partir de um batch de √≠ndices, que no caso, ser√° a sequ√™ncia recebida. Isso ocorre porque ao multiplicar uma das linhas da sequ√™ncia ap√≥s One-Hot Encoding, de dimens√µes $1 \times |T|$, por uma matriz de dimens√µes $|T| \times d$, o resultado ser√° a $m$-√©sima linha dessa matriz. Essa matriz √© denominada matriz de consulta, j√° que esse comportamento √© similar a acessar uma tabela de consulta, ou acessar os elementos de uma lista ou array atrav√©s de √≠ndices.

Logo, ao multiplicar a sequ√™ncia inteira ap√≥s One-Hot Encoding, de dimens√µes $t \times d$, pela matriz de consulta, o resultado ser√° as suas linhas concatenadas de acordo com a ordem dos tokens da sequ√™ncia recebida.

No exemplo, vamos considerar que o embedding de cada token possui sempre o mesmo valor ap√≥s essa transforma√ß√£o. Nesse caso, √© poss√≠vel concatenar todos os $|T|$ embeddings poss√≠veis em ordem para criar uma matriz de consulta, e a combina√ß√£o dessas opera√ß√µes pode representar o processo de transforma√ß√£o da sequ√™ncia em embeddings.

$$
    \underbrace{
        \begin{array}{c|cccccc}
            & 0 & 1 & 2 & 3 & 4 & \dots & |T| \\
            \hline
            1 & 0 & 1 & 0 & 0 & 0 & \dots & 0 \\
            4 & 0 & 0 & 0 & 0 & 1 & \dots & 0 \\
            3 & 0 & 0 & 0 & 1 & 0 & \dots & 0 \\
            4 & 0 & 0 & 0 & 0 & 1 & \dots & 0 \\
            3 & 0 & 0 & 0 & 1 & 0 & \dots & 0 \\
            2 & 0 & 0 & 1 & 0 & 0 & \dots & 0
        \end{array}
    }_{\text{One-Hot Encoding}}
$$

$$
    \times
$$

$$
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            0      & 0.1    & 0.2    & \dots  & 0.6 \\
            1      & 0.7    & 0.8    & \dots  & 0.3 \\
            2      & 0.4    & 0.5    & \dots  & 0.9 \\
            3      & 0.9    & 0.1    & \dots  & 0.5 \\
            4      & 0.3    & 0.9    & \dots  & 0.1 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            |T|    & 0.2    & 0.4    & \dots  & 0.1
        \end{array}
    }_{\text{Matriz de consulta}}
$$

$$
    \downarrow
$$

$$
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            1  & 0.7 & 0.8 & \dots & 0.3 \\
            4  & 0.3 & 0.9 & \dots & 0.1 \\
            3  & 0.9 & 0.1 & \dots & 0.5 \\
            4  & 0.3 & 0.9 & \dots & 0.1 \\
            3  & 0.9 & 0.1 & \dots & 0.5 \\
            2  & 0.4 & 0.5 & \dots & 0.9 \\
        \end{array}
    }_{\text{Embeddings}}
$$

Al√©m de ser aceler√°vel usando √°lgebra linear, essa forma de acesso aos embeddings permite que os valores dos embeddings possam ser otimizados durante o treinamento do modelo para melhorar a sua performance. Para isso, basta tratar a matriz de consulta como se fossem os pesos de uma camada linear em uma rede neural feed-forward, e otimizar o seu valor a partir da fun√ß√£o de perda do modelo atrav√©s de backpropagation. Assim, √© poss√≠vel encontrar um espa√ßo √≥timo para representar os tokens para o conjunto de dados de treinamento, sendo necess√°rio definir apenas o valor de $d$.

Al√©m da performance, os vetores encontrados nesse espa√ßo s√£o densos, j√° que $d$, que costuma ser um valor na casa dos milhares, geralmente √© muito menor do que $|T|$, que pode chegar √†s centenas de milhares de tokens no contexto de Large Language Models. Isso faz com que o modelo requira muito menos mem√≥ria durante o seu uso e evita problemas causados pela dimensionalidade elevada presente ao treinar modelos usando One-Hot Encoding diretamente.

Em PyTorch, esse componente est√° encapsulado na classe `torch.nn.Embedding`.

```python
embed_dim = 512

embedder = nn.Embedding(
    num_embeddings=len(printable) + 1,
    embedding_dim=embed_dim,
)

embedder(42)
```

### Token embeddings em PyTorch

```python
class TokenEmbedder(nn.Module):
    def __init__(self: Self, vocab: str, embed_dim: int) -> None:
        super().__init__()
        self.vocab = set(vocab)
        self.embed_dim = embed_dim

        self.char_ids = {
            '<pad>': 0,
            '<bos>': 1,
            '<eos>': 2,
        }

        self.char_ids.update({
            char: index + 3
            for index, char in enumerate(self.vocab)
        })

        self.embedding = nn.Embedding(
            num_embeddings=len(self.vocab) + 3,
            embedding_dim=self.embed_dim,
        )

    def tokenize(self: Self, chars: str) -> list[int]:
        tokens = [1]
        tokens.extend(self.char_ids[char] for char in chars)
        return [*tokens, 2]

    def forward(self: Self, inputs: list[str]) -> torch.Tensor:
        chars, *inputs = inputs

        tokens = self.tokenize(chars)
        token_lists = [tokens]

        token_count = len(tokens)
        token_counts = [token_count]

        max_length = token_count

        for chars in inputs:
            tokens = self.tokenize(chars)
            token_lists.append(tokens)

            token_count = len(tokens)
            token_counts.append(token_count)

            max_length = max(max_length, token_count)

        for tokens, token_count in zip(token_lists, token_counts):
            tokens.extend(0 for _ in range(max_length - token_count))

    tokens_tensor = torch.tensor(token_lists)
    embeddings = self.embedding(tokens_tensor)
    return embeddings, token_counts
```

## Aten√ß√£o

No contexto de redes neurais para transdu√ß√£o de sequ√™ncias, o uso de aten√ß√£o, de forma geral, se refere √† capacidade do modelo de entender o contexto de cada elemento da sequ√™ncia ao gerar um novo valor para cada elemento recebido.

Nesse processo, os scores de aten√ß√£o s√£o gerados, uma sequ√™ncia intermedi√°ria representando a intensidade com que cada elemento recebido deve ser utilizado para gerar os novos elementos individualmente. Ao combinar os scores de aten√ß√£o com os elementos originais, a sequ√™ncia gerada √© obtida.

O uso de mecanismos de aten√ß√£o dessa forma permite que modelos compreendam o contexto onde cada palavra est√° inserida usando menos opera√ß√µes, o que acelera treinamentos e pode otimizar os modelos (por evitar que n√£o entendam textos amb√≠guos sem considerar o contexto).

Por exemplo, considere o texto a seguir:

> Jo√£o pensou: O trem estava cheio, mas ele conseguiu uma cadeira livre.

Por simplicidade, ignore os tokens especiais e suponha que cada palavra √© representada por um embedding (pelo processo abstra√≠do por $\text{Emb}(X)$), transformando o texto na sequ√™ncia a seguir:

$$
    \begin{array}{ccccccccc}
        \text{Jo√£o } & \text{pensou: } & \text{O }  & \text{trem } & \text{estava } & \text{cheio, } & \text{mas } & \text{ele } & \cdots \\
        \quad         \\
        \downarrow   & \downarrow      & \downarrow & \downarrow   & \downarrow     & \downarrow     & \downarrow  & \downarrow   \\
        \quad         \\
        64           & 23              & 28         & 91           & 12             & 44             & 57          & 72           \\
        \quad         \\
        \downarrow   & \downarrow      & \downarrow & \downarrow   & \downarrow     & \downarrow     & \downarrow  & \downarrow   \\
        \quad         \\
        0.02         & 0.68            & 0.46       & 1.49         & 0.6            & 1.36           & 0.7         & 0.38         \\
        0.12         & 1.05            & 1.7        & 1.59         & 0.88           & 0.3            & 1.85        & 0.94         \\
        0.13         & 1.57            & 0.75       & 0.69         & 0.7            & 1.75           & 0.7         & 0.63         \\
        \cdots       & \cdots          & \cdots     & \cdots       & \cdots         & \cdots         & \cdots      & \cdots      &        \\
        0.8          & 0.34            & 0.84       & 0.34         & 0.67           & 0.53           & 0.49        & 0.5
    \end{array}
$$

Ent√£o, um mecanismo de aten√ß√£o recebe essa sequ√™ncia e gera outra de mesmo comprimento, onde o embedding que representa a palavra "ele" ser√° composto de embeddings mais pr√≥ximos do trecho que comp√µe a palavra "Jo√£o" do que da palavra "√¥nibus":

$$
    \underbrace{
        \begin{array}{ccccccccc}
            \text{Jo√£o pensou: o trem estava cheio, mas ele...}
        \end{array}
    }_{X}
$$

$$
    \downarrow
$$

$$
    \underbrace{
        \begin{bmatrix}
            0.02   & 0.68   & 0.46   & 1.49   & 0.6    & 1.36   & 0.7    & 0.38   & \cdots & 1.23   \\
            0.12   & 1.05   & 1.7    & 1.59   & 0.88   & 0.3    & 1.85   & 0.94   & \cdots & 0.11   \\
            0.13   & 1.57   & 0.75   & 0.69   & 0.7    & 1.75   & 0.7    & 0.63   & \cdots & 1.04   \\
            \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
            0.8    & 0.34   & 0.84   & 0.34   & 0.67   & 0.53   & 0.49   & 0.5    & \cdots & 0.09
        \end{bmatrix}
    }_{\text{Emb}(X)}
$$

$$
    \downarrow
$$

$$
\underbrace{
    \begin{bmatrix}
        0.02   & 0.68   & 0.46   & 1.49   & 0.6    & 1.36   & 0.7    & 0.05   & \cdots 1.23   \\
        0.12   & 1.05   & 1.7    & 1.59   & 0.88   & 0.3    & 1.85   & 0.08   & \cdots 0.11   \\
        0.13   & 1.57   & 0.75   & 0.69   & 0.7    & 1.75   & 0.7    & 0.16   & \cdots 1.04   \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \vdots \\
        0.8    & 0.34   & 0.84   & 0.34   & 0.67   & 0.53   & 0.49   & 0.11   & \cdots 0.09
    \end{bmatrix}
}_{\text{Atn}(X)}
$$

$$
    \text{Atn} \circ \text{Emb } (\text{ele}) \approx \text{Atn} \circ \text{Emb } (\text{Jo√£o})
$$

### Self-Attention

Todos os mecanismos que ser√£o usados na arquitetura dos Transformers s√£o baseados apenas em combinar os elementos da sequ√™ncia recebida entre si para determinar a aten√ß√£o para cada elemento.

Por√©m, nem todos os mecanismos funcionam dessa forma. Outros dependem de recursos externos para determinar a aten√ß√£o, como outras vari√°veis ou modelos. Por isso, √© dito que os mecanismos explicados usam Self-Attention, o que significa que assumem que a aten√ß√£o para cada elemento pode ser determinada corretamente sem a necessidade de usar outros recursos externos.

### Queries, Keys, Values

Para explicar conceitualmente o funcionamento desses mecanismos, embora os termos dessas opera√ß√µes sejam numericamente iguais no primeiro momento, alguns termos receber√£o nomes abstratos diferentes.

De forma reducionista, um mecanismo de aten√ß√£o funciona de como um dicion√°rio em Python, onde chaves (keys ou $K$) s√£o associadas a valores (values ou $V$) e √© poss√≠vel recuperar um valor posteriormente a partir da sua chave, denominada consulta (query ou $Q$). A analogia √© que no contexto do mecanismo, quem exerce esses pap√©is s√£o:

* Query: Um elemento da sequ√™ncia recebida, como a palavra "ele".
* Keys: Ser√£o todos os elementos originais da sequ√™ncia recebida
* Value: Ser√° algum elemento da sequ√™ncia recebida.
  * Caso seja necess√°rio entender algum contexto, ser√° um elemento que melhor representa a query, como a palavra "Jo√£o" no caso da palavra "ele".
  * Caso contr√°rio, ser√° a pr√≥pria query.

O exemplo a seguir ilustra essa analogia:

```python
sequence = keys = values = [
    'Jo√£o',
    'pensou',
    'O',
    'trem',
    'estava',
    'cheio',
    'mas',
    'ele',
    ...
]

mechanism = AttentionMechanism(keys, values)
query = 'ele'

assert mechanism[query] == 'Jo√£o'
```

Uma das grandes diferen√ßas entre as duas estruturas √© que retornar apenas um elemento para cada query pode n√£o ser o suficiente para representar o contexto corretamente em situa√ß√µes mais amb√≠guas. Por exemplo:

> Vi Jo√£o e Maria ontem. Eles estavam juntos.

Nesse caso, a palavra "Eles" n√£o pode ser compreendida corretamente usando apenas um value. Por isso, um mecanismo de aten√ß√£o n√£o retornar√° apenas um valor, e sim uma fra√ß√£o do quanto cada um dos values poss√≠veis deve ser considerado:

```python
sequence = keys = values = [
    'Jo√£o',
    'pensou',
    'O',
    'trem',
    'estava',
    'cheio',
    'mas',
    'ele',
    ...
]

mechanism = AttentionMechanism(keys, values)
query = 'ele'

assert mechanism[query] == {
    'Jo√£o': 0.9,
    'pensou': 0.01,
    'O': 0.01,
    'trem': 0.01,
    'estava': 0.01,
    'cheio': 0.01,
    'mas': 0.01,
    'ele': 0.01,
    ...
}
```

Essas fra√ß√µes ser√£o os scores de aten√ß√£o e s√£o uma sequ√™ncia normalizada, ou seja, a soma de todos os elementos √© 1. Como os scores s√£o normalizados e nos Transformers as sequ√™ncias recebidas ser√£o representadas por embeddings, √© poss√≠vel aplicar uma m√©dia ponderada usando os scores para gerar um novo elemento, que ser√° a sequ√™ncia gerada pelo mecanismo de aten√ß√£o.

Os scores precisam ser normalizados para que a aten√ß√£o que pode ser distribu√≠da seja finita e o mecanismo funcione corretamente. Ent√£o, Se o score de um elemento for relativamente maior, o score de pelo menos um dos outros elementos ser√° relativamnnte menor de forma proporcional para manter essa propriedade.

$$
    \text{Atn}(\text{Ele}) = 0.9 \cdot \text{Emb} (\text{Jo√£o}) + 0.01 \cdot \text{Emb} (\text{pensou: }) + \cdots
$$

$$
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            1  & 0.7 & 0.8 & \dots & 0.3 \\
            2  & 0.3 & 0.2 & \dots & 0.2 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            t  & 0.2 & 0.4 & \dots & 0.1
        \end{array}
    }_{V}
$$

$$
    \times
$$

$$
\underbrace{
    \begin{array}{c|cccc}
        & 1 & 2 & \dots & t \\
        \hline
        1  & 0.5 & 0.5 & \dots & 0.0 \\
        2  & 0.2 & 0.3 & \dots & 0.4 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        t  & 0.1 & 0.2 & \dots & 0.5
    \end{array}
}_{\text{Scores de aten√ß√£o}}
$$

$$
    \downarrow
$$

$$
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            1  & 0.4 & 0.3 & \dots & 0.8 \\
            2  & 0.9 & 0.1 & \dots & 0.1 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            t  & 0.2 & 0.5 & \dots & 0.3
        \end{array}
    }_{\text{Atn}(X)}
$$

Como nos Transformers ser√° necess√°rio transformar cada elemento da sequ√™ncia recebida, o mecanismo tamb√©m poder√° ser acelerado ao ser realizado em batch, fazendo com que as queries tamb√©m sejam iguais √† sequ√™ncia recebida.

$$
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            1  & 0.7 & 0.8 & \dots & 0.3 \\
            2  & 0.3 & 0.2 & \dots & 0.2 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            t  & 0.2 & 0.4 & \dots & 0.1
        \end{array}
    }_{\text{Q}}
    \qquad
    \underbrace{
        \begin{array}{c|cccc}
            & 1 & 2 & \dots & d \\
            \hline
            1  & 0.7 & 0.8 & \dots & 0.3 \\
            2  & 0.3 & 0.2 & \dots & 0.2 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            t  & 0.2 & 0.4 & \dots & 0.1
        \end{array}
    }_{\text{K}}
$$

$$
    \downarrow
$$

$$
    \underbrace{
    \begin{array}{c|cccc}
        & 1 & 2 & \dots & t \\
        \hline
        1  & 0.5 & 0.5 & \dots & 0.0 \\
        2  & 0.2 & 0.3 & \dots & 0.4 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        t  & 0.1 & 0.2 & \dots & 0.5
    \end{array}
    }_{\text{Scores de aten√ß√£o}}
$$

Por isso, embora queries, keys e values partam da mesma sequ√™ncia inicialmente, √© importante separar o seu papel em cada parte do mecanismo.

### Mecanismos de aten√ß√£o

#### Dot-Product Attention (DPA)

Nesse mecanismo, os scores de aten√ß√£o para cada elemento s√£o determinados a partir do produto interno entre queries e keys.

$$
  \text{Scores-DPA}(Q,K) = Q^TK
$$

Calculando a nova sequ√™ncia em batch, o DPA poder√° ser definido como:

$$
  \text{DPA}(Q,K,V) = \text{Scores-DPA}(Q,K) \cdot V = Q^TKV
$$

Por√©m, o produto interno de dois vetores n√£o est√° contido entre 0 e 1, e sim entre $-\infty$ e $\infty$. Dessa forma, o mecanismo poder√° atribuir aten√ß√£o infinitamente entre todos os elementos, o que pode enviesar o modelo e inviabilizar o uso do mecanismo. Para corrigir isso e normalizar a aten√ß√£o, √© aplicada a fun√ß√£o softmax sobre os scores:

$$
  \text{DPA}(Q,K,V) = \text{Softmax}(Q^TK)V
$$

#### Scaled Dot-Product Attention (SDPA)

Essa varia√ß√£o do DPA adiciona um termo de normaliza√ß√£o sobre os scores para estabilizar os gradientes gerados pelo mecanismo durante a etapa de backpropagation:

$$
  \text{SDPA}(Q,K,V) = \text{Softmax}\left(\frac{Q^TK}{\sqrt{d}}\right)V
$$

A origem da normaliza√ß√£o por $\sqrt{d}$ √© emp√≠rica. Antes dos Transformers, j√° se observava experimentalmente que normalizar a vari√¢ncia do gradiente gerado por camadas ocultas evita problemas de gradient vanishing e neur√¥nios mortos durante o treinamento de modelos, caracter√≠stica que foi adaptada para a arquitetura posteriormente.

#### Proje√ß√µes lineares

Uma das formas com que os Transformers tornam o SDPA mais eficiente √© projetar linearmente as queries, keys e values para espa√ßos diferentes, multiplicando-as por matrizes de par√¢metros trein√°veis (denotadas como $W^Q$, $W^K$ e $W^V$). Isso faz com que os valores das queries, keys e values se tornem diferentes entre si, e durante o treino, esses par√¢metros sejam otimizados para otimizar a forma como o transformer converte os embeddings originais em outro espa√ßo com as mesmas dimens√µes, mas que representam melhor o valor de cada token no contexto onde est√£o inseridos.

$$
  \text{SDPA-Transformer}(Q,K,V) =  SDPA(QW^Q, KW^K,VW^V)
$$

#### Multihead Attention (MHA)

Essa varia√ß√£o do SDPA consiste em aplicar o mecanismo $h$ vezes sobre os embeddings para combinar os resultados dessas aplica√ß√µes, onde o n√∫mero de cabe√ßas $h$ √© um hiperpar√¢metro do modelo.

Como os scores no SDPA s√£o normalizados, n√£o √© poss√≠vel prestar aten√ß√£o em todos os elementos simultaneamente. O objetivo do uso de MHA √© fazer com que cada SDPA foque em um tipo de padr√£o diferente no representar do contexto para criar um modelo melhor durante o treinamento.

Usando uma analogia, MHA seria o equivalente a ler um texto $h$ vezes, permitindo prestar aten√ß√£o em partes diferentes do texto a cada vez para entender melhor o contexto de cada palavra. A diferen√ßa √© que cada leitura √© feita simultaneamente no mecanismo.

Embora esse mecanismo seja eficaz em otimizar modelos, se for implementado da forma literal, √© necess√°rio calcular SDPA $h$ vezes, tornando o MHA $h$ vezes mais lento, algo que n√£o √© desejado, j√° que um dos dos objetivos dos Transformers √© criar modelos eficientes computacionalmente. Por isso, √© usado o algoritmo alternativo a seguir para fazer com que a escalabilidade do mecanismo n√£o seja afetada pelo n√∫mero de cabe√ßas:

1. Dividir as queries, keys e values em $h$ trechos cont√≠guos, transformando as dimens√µes do batch de $b \times t \times d$ para $b \times t \times h \times \frac{h}{d}$.
2. Reordenar a ordem dos elementos das queries, keys e values, transformando as dimens√µes do batch de $b \times t \times h \times \frac{h}{d}$ para $b \times h \times t \times \frac{h}{d}$.
3. Aplicar SDPA no batch $h$ vezes, usando proje√ß√£o diferentes para cada cabe√ßa.
4. Concatenar os embeddings de cada cabe√ßa.
5. Restaurar a ordem dos elementos no batch gerado, restaurando as dimens√µes para $b \times t \times d$.
6. Aplicar uma proje√ß√£o linear $W^O$ nos embeddings concatenados.

Ap√≥s a etapa 3, o algoritmo pode ser descrito pela equa√ß√£o a seguir:

$$
  \text{MHA}(Q,K,V) = \left(\Big \Vert^h_{i=1} \text{SDPA}(QW^Q_i, KW^K_i,VW^V_i) \right) W^O
$$

Dessa forma, SDPA ainda √© aplicado $h$ vezes, mas como os elementos possuem dimens√µes menores, cada uma √© $\frac{1}{h}$ vezes mais r√°pida, o que torna a escalabilidade do SDPA e MHA igual.

De forma pr√°tica, aplicar MHA ainda ser√° mais lento que aplicar SDPA devido √† proje√ß√£o linear $W^O$. Por√©m, o tempo adicionado n√£o aumenta em rela√ß√£o a nenhuma vari√°vel.

##### MHA em PyTorch

```python
class MultiheadAttention(nn.Module):
    def __init__(self: Self, embed_dim: int, n_heads: int) -> None:
        super().__init__()

        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.head_dim = self.embed_dim // self.n_heads

        self.query_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.key_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.value_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.output_projection = nn.Linear(embed_dim, embed_dim, bias=False)

    def split_embeddings(
        self: Self,
        inputs: torch.Tensor,
        batches: int,
        tokens: int,
    ) -> torch.Tensor:
        split_inputs = inputs.view(batches, tokens, self.n_heads, self.head_dim)
        head_sorted_inputs = split_inputs.transpose(1, 2)
        return head_sorted_inputs

    def join_embeddings(
        self: Self,
        inputs: torch.Tensor,
        batches: int,
        tokens: int,
    ) -> torch.Tensor:
        token_sorted_inputs = inputs.transpose(1, 2)
        token_sorted_inputs = token_sorted_inputs.contiguous()
        joined_inputs = token_sorted_inputs.view(batches, tokens, self.embed_dim)
        return joined_inputs

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        batches, tokens, _ = queries.size()

        queries = self.query_projection(queries)
        keys = self.key_projection(keys)
        values = self.value_projection(values)

        queries = self.split_embeddings(queries, batches, tokens)
        keys = self.split_embeddings(keys, batches, tokens)
        values = self.split_embeddings(values, batches, tokens)

        keys = keys.transpose(2, 3)

        scores = queries @ keys / (self.head_dim**0.5)

        if mask is not None:
            scores = scores.masked_fill(mask, float("-inf"))
            print(scores)

        weights = F.softmax(scores, dim=3)

        attn_outputs = weights @ values
        joined_outputs = self.join_embeddings(attn_outputs, batches, tokens)
        projected_outputs = self.output_projection(joined_outputs)

        return projected_outputs
```

## Positional Encoding (PE)

Nenhum dos mecanismos de aten√ß√£o explicados considera a posi√ß√£o dos elementos na sequ√™ncia recebida na sequ√™ncia gerada. Isso significa que alterar a ordem dos elementos n√£o afeta o resultado, um efeito indesejado e que pode enviesar modelos durante o treinamento.

Antes da aplica√ß√£o de qualquer mecanismo de aten√ß√£o, as posi√ß√µes dos elementos originais s√£o representadas usando uma t√©cnica de PE, que gera uma sequ√™ncia de embeddings onde cada elemento representa uma posi√ß√£o na sequ√™ncia original de alguma forma.

Esses embeddings ser√£o combinados com a sequ√™ncia recebida para alterar a sequ√™ncia recebida de forma que cada elemento tenha sua posi√ß√£o codificada individualmente.

A t√©cnica de PE introduzida na arquitetura dos Transformers √© baseada na fun√ß√£o a seguir:

$$
    \text{PE}(i, j, p) =
    \begin{cases}
        \sin \dfrac{p}{\theta^{\frac{2i}{d}}} & \text{se }j \text{ √© par}, \\
        \cos \dfrac{p}{\theta^{\frac{2i}{d}}} & \text{se }j \text{ √© √≠mpar}.
    \end{cases}
$$

Onde:

* $p$ √© a posi√ß√£o de um elemento na sequ√™ncia recebida.
* $j$ √© a posi√ß√£o de um item em um elemento da sequ√™ncia recebida.
* $0 \leq i < \frac{d}{2}$, e $i$ √© incrementado a cada dois itens consecutivos em um elemento da sequ√™ncia recebida.
* $\theta$ √© um hiperpar√¢metro.

Os embeddings gerados por $PE$ ser√£o ent√£o somados √† sequ√™ncia recebida, e essa sequ√™ncia poder√° ser usada pelos mecanismos de aten√ß√£o.

Nos Transformers, a escolha dessa fun√ß√£o foi feita por ser o equivalente a aplicar uma matriz de rota√ß√£o nos elementos da sequ√™ncia, onde o √¢ngulo de rota√ß√£o de um elemento √© determinado pela sua posi√ß√£o.

A t√©cnica de PE utilizada √© relativa, ou seja, prioriza representar a posi√ß√£o de um elemento em rela√ß√£o aos seus vizinhos em rela√ß√£o a representar a ordem dos elementos de forma absoluta.

Para ilustrar o c√°lculo de $PE$ em batch, a sequ√™ncia de embedings que ser√° somada com uma sequ√™ncia recebida onde $t = 5$, $d = 4$ e $\theta = 10000$ ser√°:

$$
    i =
    \begin{bmatrix}
        0 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1
    \end{bmatrix}
$$

$$
    j =
    \begin{bmatrix}
        0 & 1 & 2 & 3 \\
        0 & 1 & 2 & 3 \\
        0 & 1 & 2 & 3 \\
        0 & 1 & 2 & 3 \\
        0 & 1 & 2 & 3
    \end{bmatrix}
$$

$$
    p =
    \begin{bmatrix}
        0 & 0 & 0 & 0 \\
        1 & 1 & 1 & 1 \\
        2 & 2 & 2 & 2 \\
        3 & 3 & 3 & 3 \\
        4 & 4 & 4 & 4
    \end{bmatrix}
$$

$$
    PE(i,j,p) =
    \begin{bmatrix}
        0.0000 & 1.0000 & 0.0000 & 1.0000 \\
        0.8415 & 0.5403 & 0.0100 & 0.9999 \\
        0.9093 & -0.4161 & 0.0200 & 0.9998 \\
        0.1411 & -0.9899 & 0.0300 & 0.9996 \\
        -0.7568 & -0.6536 & 0.0400 & 0.9992 \\
    \end{bmatrix}
$$

### PE em PyTorch

```python
class PositionalEncoder(nn.Module):
    def __init__(self: Self, embed_dim: int, theta: int) -> None:
        super().__init__()
        self.embed_dim = embed_dim
        self.theta = theta

    @torch.no_grad()
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        batches, tokens, _ = inputs.size()

        indexes = torch.arange(self.embed_dim, dtype=torch.float)

        positions = torch.arange(tokens, dtype=torch.float)
        positions = positions.view(tokens, 1)

        i = torch.arange(self.embed_dim // 2)
        i = i.float()
        i = i.repeat_interleave(2)

        cos_indexes = indexes % 2
        cos_indexes = cos_indexes.bool()
        cos_indexes = cos_indexes.expand((tokens, self.embed_dim))

        sin_indexes = ~cos_indexes

        encodings = positions / (self.theta ** (2 * i / self.embed_dim))

        encodings[sin_indexes] = encodings[sin_indexes].sin()
        encodings[cos_indexes] = encodings[cos_indexes].cos()

        encodings = encodings.expand((batches, tokens, self.embed_dim))

        return inputs + encodings
```

## Modelos autoregressivos

Transformers s√£o modelos que realizam transdu√ß√£o de sequ√™ncias de forma autoregressiva, caracter√≠stica que dita como todos os componentes ser√£o combinados.

Quando um modelo de transdu√ß√£o de sequ√™ncias recebe uma sequ√™ncia, se esse modelo for autoregressivos, ent√£o ele ser√° treinado para alterar essa sequ√™ncia realizando um processo denominado shift, onde:

* O primeiro elemento ser√° descartado.
* Todos os elementos ser√£o deslocados uma posi√ß√£o para tr√°s.
* Um novo elemento, gerado pelo modelo, preencher√° a √∫ltima posi√ß√£o.

O √∫ltimo elemento gerado pelo modelo ser√° o primeiro elemento da sequ√™ncia gerada. Ent√£o, essa sequ√™ncia p√≥s shift ser√° a nova sequ√™ncia recebida, e o resultado ser√° um novo elemento para a sequ√™ncia gerada, e assim por diante, at√© atingir um crit√©rio de parada.

Entre os crit√©rios de parada, √© poss√≠vel:

1. Definir um n√∫mero m√°ximo de itera√ß√µes
2. Encerrar o algoritmo quando o √∫ltimo elemento gerado for igual a um valor especial (como o token `<eos>`).

O exemplo a seguir ilustra como a tradu√ß√£o autoregressiva "cachorro" em Portugu√™s para a palavra "dog" em Ingl√™s. Para isso, os tokens especiais separam o texto recebido do texto gerado, que ser√° representado como `<bos>cachorro<eos><bos>dog<eos>`.

$$
\underbrace{
 \begin{array}{c|cccccccccc}
    & 0              & 1              & 2              & 3              & 4              & 5              & 6              & 7              & 8              & 9              \\\hline
  1 & \texttt{<bos>} & \texttt{  c  } & \texttt{  a  } & \texttt{  c  } & \texttt{  h  } & \texttt{  o  } & \texttt{  r  } & \texttt{  r  } & \texttt{  o  } & \texttt{<eos>} \\
  2 & \texttt{  c  } & \texttt{  a  } & \texttt{  c  } & \texttt{  h  } & \texttt{  o  } & \texttt{  r  } & \texttt{  r  } & \texttt{  o  } & \texttt{<eos>} & \texttt{<bos>} \\
  3 & \texttt{  a  } & \texttt{  c  } & \texttt{  h  } & \texttt{  o  } & \texttt{  r  } & \texttt{  r  } & \texttt{  o  } & \texttt{<eos>} & \texttt{<bos>} & \texttt{  d  } \\
  4 & \texttt{  c  } & \texttt{  h  } & \texttt{  o  } & \texttt{  r  } & \texttt{  r  } & \texttt{  o  } & \texttt{<eos>} & \texttt{<bos>} & \texttt{  d  } & \texttt{  o  } \\
  5 & \texttt{  h  } & \texttt{  o  } & \texttt{  r  } & \texttt{  r  } & \texttt{  o  } & \texttt{<eos>} & \texttt{<bos>} & \texttt{  d  } & \texttt{  o  } & \texttt{  g  } \\
 \end{array}
 }_{\text{Sequ√™ncia original}}
$$

$$
\downarrow
$$

$$
\underbrace{
    \begin{array}{c|cccccccccc}
        & 0            & 1            & 2            & 3            & 4              & 5              & 6              & 7              & 8              & 9              \\
        \hline
        1 & \texttt{ c } & \texttt{ a } & \texttt{ c } & \texttt{ h } & \texttt{ o }   & \texttt{ r }   & \texttt{ r }   & \texttt{ o }   & \texttt{<eos>} & \texttt{<bos>} \\
        2 & \texttt{ a } & \texttt{ c } & \texttt{ h } & \texttt{ o } & \texttt{ r }   & \texttt{ r }   & \texttt{ o }   & \texttt{<eos>} & \texttt{<bos>} & \texttt{ d }   \\
        3 & \texttt{ c } & \texttt{ h } & \texttt{ o } & \texttt{ r } & \texttt{ r }   & \texttt{ o }   & \texttt{<eos>} & \texttt{<bos>} & \texttt{ d }   & \texttt{ o }   \\
        4 & \texttt{ h } & \texttt{ o } & \texttt{ r } & \texttt{ r } & \texttt{ o }   & \texttt{<eos>} & \texttt{<bos>} & \texttt{ d }   & \texttt{ o }   & \texttt{ g }   \\
        5 & \texttt{ o } & \texttt{ r } & \texttt{ r } & \texttt{ o } & \texttt{<eos>} & \texttt{<bos>} & \texttt{ d }   & \texttt{ o }   & \texttt{ g }   & \texttt{<eos>}
    \end{array}
}_{\text{Sequ√™ncia ap√≥s o shift}}
$$

Note que o primeiro shift sempre ter√° o mesmo resultado: mover o token `<bos>` do in√≠cio para o final do texto. Esse padr√£o ser√° √∫til no treinamento dos Transformers futuramente.

Al√©m disso, note que o tamanho da sequ√™ncia gerada √© igual ao n√∫mero de shifts realizados, valor independente do tamanho da sequ√™ncia recebida. Logo, √© poss√≠vel gerar sequ√™ncias de qualquer tamanho usando modelos autoregressivos.

### Attention mask

Todos os mecanismos de aten√ß√£o explicados usam todos os elementos da sequ√™ncia recebida para gerar um novo elemento. Logo, o i-√©simo elemento da sequ√™ncia gerada ser√° baseado no i+1-√©simo elemento, no i+2-√©simo elemento e assim por diante. Durante o treinamento, essa caracter√≠stica pode enviesar o modelo.

Esse vi√©s ocorre porque a fun√ß√£o de perda ser√° calculada comparando a sequ√™ncia p√≥s shift e a sequ√™ncia gerada, ou seja, para todos os elementos exceto o √∫ltimo, a perda ser√° baseada em comparar se o i-√©simo elemento da sequ√™ncia gerada se tornou igual ao i+1-√©simo elemento da sequ√™ncia recebida. Portanto, se o i+1-√©simo elemento puder ser considerado durante a gera√ß√£o, o modelo sempre usar√° apenas esse valor.

Esse padr√£o √© um vazamento de dados, e se n√£o for corrigido, o modelo possui alto potencial de n√£o conseguir gerar o √∫ltimo elemento corretamente. Por isso, √© necess√°rio limitar o acesso do modelo aos elementos posteriores durante a gera√ß√£o para que consiga identificar os padr√µes de aten√ß√£o corretamente durante o treinamento.

Essa limita√ß√£o √© feita nos Transformers aplicando uma attention mask, que anula parte dos scores de forma que um novo elemento n√£o ser√° gerado usando elementos posteriores.

A aplica√ß√£o da attention mask consiste em somar $-\infty$ aos elementos da matriz triangular superior dos scores de aten√ß√£o. Dessa forma, esses elementos ter√£o valor 0 ap√≥s a aplica√ß√£o da Softmax.

$$
    \text{SDPA-Mask}(Q,K,V,M) = \text{Softmax}\left(\frac{Q^TK}{\sqrt{d_{in}}}+M\right)V
$$

$$
    \underbrace{
    \begin{array}{c|ccccc}
        & 1      & 2      & 3      & \dots  & t-1    & t      \\
    \hline
    1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
    2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
    3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
    t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
    \end{array}
    }_{\text{Aten√ß√£o original}}
$$

$$
    +
$$

$$
    \underbrace{
        \begin{array}{c|ccccc}
            & 1      & 2          & 3        & \dots  & t-1      & t        \\
        \hline
        1        & 0      & -\infty  & -\infty  & \dots  & -\infty  & -\infty  \\
        2        & 0      & 0        & -\infty  & \dots  & -\infty  & -\infty  \\
        3        & 0      & 0        & 0        & \dots  & -\infty  & -\infty  \\
        \vdots   & \vdots & \vdots   & \vdots   & \ddots & \vdots   & \vdots   \\
        t-1      & 0      & 0        & 0        & \dots  & 0        & -\infty  \\
        t        & 0      & 0        & 0        & \dots  & 0        & 0
        \end{array}
    }_{\text{M√°scara de aten√ß√£o}}
$$

$$
    \downarrow
$$

$$
    \underbrace{
        \begin{array}{c|ccccc}
            & 1      & 2      & 3      & \dots  & t-1    & t      \\
        \hline
        1      & 6.32   & -\infty & -\infty    & \dots  & -\infty  & -\infty  \\
        2      & -1.81  & 0.96    & -\infty    & \dots  & -\infty  & -\infty  \\
        3      & -1.81  & 0.06    & -2.27      & \dots  & -\infty  & -\infty  \\
        \vdots & \vdots & \vdots  & \vdots     & \ddots & \vdots   & \vdots   \\
        t-1    & -0.11  & 1.55    & -0.18      & \dots  & 0.95     & -\infty  \\
        t      & 1.45   & -1.42   & 1.62       & \dots  & 2.06     & -0.23
        \end{array}
    }_{\text{Aten√ß√£o mascarada}}
$$

Existem casos onde esse vazamento n√£o ser√° um problema, e nesse caso, a attention mask aplicada ser√° apenas um tensor nulo.

$$
    \underbrace{
        \begin{array}{c|ccccc}
            & 1      & 2      & 3      & \dots  & t-1    & t      \\
        \hline
        1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
        2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
        3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
        t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
        \end{array}
    }_{\text{Aten√ß√£o original}}
$$

$$
    +
$$

$$
    \underbrace{
        \begin{array}{c|ccccc}
                   & 1      & 2      & 3      & \dots  & t-1    & t      \\
            \hline
            1      & 0      & 0      & 0      & \dots  & 0      & 0      \\
            2      & 0      & 0      & 0      & \dots  & 0      & 0      \\
            3      & 0      & 0      & 0      & \dots  & 0      & 0      \\
            \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
            t-1    & 0      & 0      & 0      & \dots  & 0      & 0      \\
            t      & 0      & 0      & 0      & \dots  & 0      & 0
        \end{array}
    }_{\text{M√°scara de aten√ß√£o}}
$$

$$
    \downarrow
$$

$$
    \underbrace{
        \begin{array}{c|ccccc}
            & 1      & 2      & 3      & \dots  & t-1    & t      \\
        \hline
        1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
        2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
        3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
        t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
        \end{array}
    }_{\text{Aten√ß√£o original}}
$$

#### Attention Mask em PyTorch

```python
def get_attn_mask(size: int | tuple[int]) -> torch.Tensor:
    mask = torch.ones(size)
    mask = mask.triu(diagonal=1)
    mask = mask.bool()
    return mask
```

## Componentes da arquitetura

A arquitetura dos Transformers original √© composta dos componentes apresentados na seguinte ordem:

```mermaid
flowchart
    direction TB
    input(Sequ√™ncia recebida)
    inputProcessing(Processamento de entrada)
    encoder("Encoder") 
    decoder("Decoder")
    outputProcessing(Processamento de sa√≠da)
    conditional{"`eos?`"}
    input --> inputProcessing -.-> encoder -.-> decoder -.-> outputProcessing
    inputProcessing --> decoder
    outputProcessing --> conditional -- N√£o --> inputProcessing
    conditional -- Sim --> output(Sa√≠da)
```

Para ilustrar o funcionamento de cada componente da arquitetura, considere que o modelo sendo descrito √© um modelo de linguagem. Nesse caso, as sequ√™ncias recebidas sempre usar√£o o formato `<bos><sequ√™ncia recebida><eos><bos><sequ√™ncia gerada><eos>`.

### Processamento de entrada

O processamento de entrada seguir√° o processo apresentado, de cria√ß√£o de token embeddings a partir de um texto, em ordem.

```mermaid
flowchart
    lastToken(√öltimo elemento gerado)
    input(Sequ√™ncia recebida)
    
    subgraph p[ ]
    shift(Shift)
    embedding(Embedding)
    pose(PE)
    tokenizer(Tokenizer)
    end
    
    encoder(Encoder)
    decoder(Decoder)

    input ~~~ lastToken

    input -.-> tokenizer
    input & lastToken --> shift --> tokenizer

    tokenizer -.-> embedding
    tokenizer --> embedding

    embedding -.-> pose
    embedding --> pose

    pose -.-> encoder
    pose --> decoder

    decoder --> lastToken
```

A partir do texto, duas sequ√™ncias ser√£o geradas. A primeira, que ser√° a entrada do encoder, parte da sequ√™ncia recebida, e a segunda, que ser√° a entrada do decoder, parte da sequ√™ncia recebida p√≥s shift.

Usar o resultado do shift √© poss√≠vel porque na primeira itera√ß√£o, o resultado do shift √© previs√≠vel, e nas pr√≥ximas itera√ß√µes, o resultado da √∫ltima itera√ß√£o ser√° a sequ√™ncia p√≥s shift.

Por exemplo, se o √∫nico texto recebido for `cachorro` e o gerado for `dog`, na primeira itera√ß√£o, e as sequ√™ncias ser√£o:

* Entrada do encoder: `<bos>cachorro<eos>`
* Entrada do decoder: `cachorro<eos><bos>`

Na segunda itera√ß√£o, as sequ√™ncias ser√£o:

* Entrada do encoder: `cachorro<eos><bos>`
* Entrada do decoder: `cachorro<eos><bos>d`

E assim por diante.

#### Processamento de entrada em PyTorch

```python
class InputProcessor(nn.Module):
    def __init__(
        self: Self,
        tokenizer: Tokenizer,
        positional_encoder: PositionalEncoder,
    ) -> None:
        super().__init__()
        self.tokenizer = tokenizer
        self.positional_encoder = positional_encoder

    def forward(self: Self, inputs: list[str]) -> torch.Tensor:
        input_tensors, token_counts = self.tokenizer(inputs)
        encoded_tensors = self.positional_encoder(input_tensors)
        return encoded_tensors, token_counts
```

### Transformer blocks

O encoder e o decoder ser√£o baseados em Transformer blocks, que geram outra sequ√™ncia de embeddings intermedi√°ria. Esses componentes funcionam da seguinte forma:

1. Queries, Keys e Values s√£o transformados via MHA.
2. A sequ√™ncia transformada √© somada com os Values.
3. A soma √© normalizada via LayerNorm.
4. A soma normalizada √© transformada uma sequ√™ncia intermedi√°ria por uma rede neural feed-forward.
5. A sequ√™ncia gerada √© somada com a soma normalizada.
6. A segunda soma √© normalizada via LayerNorm.

```mermaid
flowchart
    Key(Keys)
    Mask(M√°scara de aten√ß√£o)    
    Query(Queries)
    Value(Values)
    Sum1("`\+`")
    Sum2("`\+`")
    MHA(MHA)
    LayerNorm1(LayerNorm)
    LayerNorm2(LayerNorm)
    Linear1(Linear)
    Linear2(Linear)
    ReLU(ReLU)
    output(Sa√≠da)
    Mask --> MHA
    Query --> MHA
    Key --> MHA
    Value --> MHA
    MHA --> Sum1
    Sum1 --> LayerNorm1
    LayerNorm1 --> Linear1
    Linear1 --> ReLU
    ReLU --> Linear2
    LayerNorm1 --> Sum2
    Linear2 --> Sum2
    Sum2 --> LayerNorm2
    LayerNorm2 --> output
    Value --> Sum1
```

As etapas 2 e 5, onde o resultado de uma camada √© somado com sua entrada, s√£o conhecidas como conex√µes residuais, uma t√©cnica introduzida com a arquitetura ResNet que tem como objetivo tornar a curva da fun√ß√£o de perda mais suave. A suaviza√ß√£o faz com que existam menos m√≠nimos locais na fun√ß√£o e a perda se aproxime do m√≠nimo global em menos passos durante o treinamento.

As camadas LayerNorm s√£o treinadas para normalizar os resultados de camadas ocultas com base na sua distribui√ß√£o para estabilizar a vari√¢ncia do gradiente gerado pela camada anterior.

A rede feed-forward usada no passo 4 √© treinada para gerar uma sequ√™ncia de embeddings em algum espa√ßo.

Os blocos sempre estar√£o em um componentes intermedi√°rios do modelo. Isso faz com que, durante o treinamento, o espa√ßo desses embeddings seja escolhido de forma que otimize o modelo, assim como o espa√ßo dos token embeddings.

A arquitetura das redes feed-forward ser√°:

1. Uma camada linear, que aumenta a dimens√£o dos elementos da sequ√™ncia para $d_{ff}$.
2. Uma fun√ß√£o de ativa√ß√£o ReLU.
3. Outra camada linear que reduz a dimens√£o dos elementos da sequ√™ncia de volta para $d$.

Onde $d_{ff}$ √© um hiperpar√¢metro.

#### Transformer blocks em PyTorch

```python
@dataclass
class TransformerLayerConfig:
    embed_dim: int = 512
    n_heads: int = 8
    hidden_dim: int = 2048


class TransformerLayer(nn.Module):
    def __init__(self: Self, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.config = config

        self.mha = MultiheadAttention(
            embed_dim=self.config.embed_dim,
            n_heads=self.config.n_heads,
        )
        self.mha_layernorm = nn.LayerNorm(normalized_shape=self.config.embed_dim)

        self.ff = nn.Sequential(
            nn.Linear(self.config.embed_dim, self.config.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.config.hidden_dim, self.config.embed_dim),
        )
        self.ff_layernorm = nn.LayerNorm(self.config.embed_dim)

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        mha_res = values

        mha_outputs = self.mha(queries, keys, values, mask)
        mha_res_outputs = mha_outputs + mha_res
        norm_mha_outputs = self.mha_layernorm(mha_res_outputs)

        ff_res = norm_mha_outputs
        ff_outputs = self.ff(norm_mha_outputs)
        norm_ff_outputs = self.ff_layernorm(ff_outputs + ff_res)

        return norm_ff_outputs
```

### Encoder

O encoder gera, a partir sequ√™ncia recebida, outra sequ√™ncia de embeddings, que auxiliar√° o decoder na gera√ß√£o dos elementos da sequ√™ncia. Esses embeddings pertencem a um espa√ßo pr√≥prio que √© determinado durante o treinamento para otimizar o decoder.

A arquitetura do decoder √© composta de $m$ blocos de decoder, o primeiro bloco usar√° a sequ√™ncia recebida como entrada, e os demais usar√£o o resultado do bloco anterior no lugar. A sequ√™ncia gerada pelo $m$¬∫ bloco ser√° considerada a sequ√™ncia gerada pelo encoder.

O valor de $m$ √© um hiperpar√¢metro que deve ser definido antes do treinamento.

```mermaid
flowchart
    input(Sequ√™ncia recebida)
    EncoderBlock(1¬∫ Bloco de transformer)
    EncoderBlock2(2¬∫ Bloco de transformer)
    EncoderBlockN(m¬∫ Bloco de transformer)
    outputN(Sa√≠da)
    input --> EncoderBlock
    EncoderBlock --> EncoderBlock2
    EncoderBlock2 -- ... --> EncoderBlockN
    EncoderBlockN --> outputN
```

#### Encoder em Pytorch

```python
class Encoder(nn.Module):
    def __init__(self: Self, n_layers: int, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.n_layers = n_layers
        self.config = config
        self.layers = nn.ModuleList(
            TransformerLayer(self.config) for _ in range(self.n_layers)
        )

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
    ) -> torch.Tensor:
        layer, *layers = self.layers
        layer_outputs = layer(
            queries=queries,
            keys=keys,
            values=values,
        )

        for layer in layers:
            layer_outputs = layer(
                queries=layer_outputs,
                keys=layer_outputs,
                values=layer_outputs,
            )
        return layer_outputs
```

### Decoder

#### Decoder em PyTorch

O decoder √© respons√°vel por combinar a sequ√™ncia p√≥s shift e a sequ√™ncia do encoder em uma sequ√™ncia final de embeddings.

A arquitetura do decoder √© composta de $n$ blocos de decoder. Todos os blocos receber√£o duas sequ√™ncias, onde uma delas ser√° a sequ√™ncia do encoder. Por√©m, o primeiro bloco tamb√©m usar√° a sequ√™ncia p√≥s shift como entrada, e os demais tamb√©m usar√£o o resultado do bloco anterior no lugar.

O valor de $n$ √© um hiperpar√¢metro que deve ser definido antes do treinamento.

```mermaid
flowchart
    input("`Sequ√™ncia p√≥s shift`")
    encoderOutput("`sequ√™ncia do encoder`")
    DecoderBlock1(1¬∫ bloco de decoder)
    DecoderBlock2(2¬∫ bloco de decoder)
    DecoderBlockN(n¬∫ bloco de decoder)
    output(Sa√≠da)

    input -- Queries, Keys, Values --> DecoderBlock1
    encoderOutput --> DecoderBlock1

    DecoderBlock1 -- Queries, Keys, Values --> DecoderBlock2
    encoderOutput --> DecoderBlock2
    
    DecoderBlock2 -- ... --> DecoderBlockN
    encoderOutput --> DecoderBlockN
    
    DecoderBlockN --> output
```

```python
class Decoder(nn.Module):
    def __init__(
        self: Self,
        n_layers: int,
        config: TransformerLayerConfig,
    ) -> None:
        super().__init__()
        self.n_layers = n_layers
        self.config = config
        self.layers = nn.ModuleList(
            DecoderLayer(self.config) for _ in range(self.n_layers)
        )

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        encoder_outputs: torch.Tensor,
    ) -> torch.Tensor:
        layer, *layers = self.layers
        layer_outputs = layer(
            queries=queries,
            keys=keys,
            values=values,
            encoder_outputs=encoder_outputs,
        )

        for layer in layers:
            layer_outputs = layer(
                queries=layer_outputs,
                keys=layer_outputs,
                values=layer_outputs,
                encoder_outputs=encoder_outputs,
            )
        return layer_outputs
```

Os blocos de decoder possuem seguinte arquitetura:

1. Queries, Keys e Values s√£o transformados via MHA usando m√°scara de aten√ß√£o.
2. A sequ√™ncia transformada e a sequ√™ncia do encoder s√£o transformados usando Encoder-Decoder Attention.

Encoder-Decoder Attention (EDA) √© um tipo de MHA e √© o √∫nico caso na arquitetura onde Queries, Keys e Values n√£o possuem o mesmo valor (j√° que recebe duas sequ√™ncias como entrada). A diferen√ßa entre EDA e MHA est√° na primeira usar os elementos da sequ√™ncia (transformada pelo passo 1) como Values e os da sequ√™ncia do encoder como Queries e Keys.

```mermaid
flowchart
    encoderOutput(Sequ√™ncia do encoder)
    Value(Values)
    Key(Keys)
    Query(Queries)
    MHA(MHA)
    DecoderBlock(Bloco de transformer)
    decoderBlockOutput(Sa√≠da)
    attentionMask(M√°scara de aten√ß√£o)
    attentionMask --> MHA
    Query --> MHA
    Key --> MHA
    Value --> MHA
    MHA -- Values --> DecoderBlock
    encoderOutput -- Queries, Keys --> DecoderBlock
    DecoderBlock --> decoderBlockOutput
```

```python
class DecoderLayer(nn.Module):
    def __init__(self: Self, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.config = config
        self.mha = MultiheadAttention(
            embed_dim=self.config.embed_dim,
            n_heads=self.config.n_heads,
        )
        self.transformer_layer = TransformerLayer(self.config)

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        encoder_outputs: torch.Tensor,
    ) -> torch.Tensor:
        batches, tokens, _ = keys.size()
        mask = get_attn_mask((batches, self.config.n_heads, tokens, tokens))
        outputs = self.mha(
            queries=queries,
            keys=keys,
            values=values,
            mask=mask,
        )
        outputs = self.transformer_layer(
            queries=encoder_outputs,
            keys=encoder_outputs,
            values=outputs,
        )
        return outputs
```

### Processamento de sa√≠da

A sequ√™ncia final de embeddings √© transformada em tokens do vocabul√°rio de sa√≠da da seguinte forma:

1. A sequ√™ncia final √© transformada linearmente, e as dimens√µes passam de $d$ para o n√∫mero de tokens no vocabul√°rio de sa√≠da.
2. Os elementos s√£o normalizados via Softmax, tornando os elementos em probabilidades para cada token poss√≠vel por posi√ß√£o na sequ√™ncia.
3. Os √≠ndices de maior probabilidade s√£o obtidos via argmax.

```mermaid
flowchart
    DecoderBlockN(Entrada) --> linear(Linear) --> Softmax(Softmax) --> Argmax(Argmax) --> outputString(Sa√≠da)
```

Cada √≠ndice obtido dessa forma representa o token que ser√° predito naquela posi√ß√£o da sequ√™ncia, e essa sequ√™ncia ser√° o resultado da itera√ß√£o do modelo autoregressivo.

Durante o treinamento, os tokens essa sequ√™ncia ser√£o comparados com os tokens esperados com  preditos para calcular a perda.

Ap√≥s o treinamento, al√©m de realizar todas as itera√ß√µes necess√°rias com o modelo autoregressivo, √© necess√°rio converter os tokens gerados para seus valores textuais equivalentes e concaten√°-los.

#### Processamento de sa√≠da em PyTorch

```python
class OutputProcessor(nn.Module):
    def __init__(self: Self, embed_dim: int, vocab: str) -> None:
        super().__init__()
        self.embed_dim = embed_dim
        self.vocab = set(vocab)
        self.out_dim = len(self.vocab)
        self.linear = nn.Linear(self.embed_dim, self.out_dim)
        self.id_chars = {index: char for index, char in enumerate(self.vocab)}

    def untokenize(self: Self, tokens: list[int]) -> str:
        return "".join(self.id_chars[token] for token in tokens)

    def forward(self: Self, inputs: torch.Tensor, token_counts: list[int]) -> list[str]:
        linear_outputs = self.linear(inputs)
        batches, tokens, _ = linear_outputs.size()

        probs = linear_outputs.softmax(dim=2)
        probs = probs.view(batches * tokens, self.out_dim)

        predictions = probs.argmax(dim=1)
        predictions = predictions.view(batches, tokens)

        outputs = [self.untokenize(tokens) for tokens in predictions.tolist()]
        outputs = [
            output[:token_count] for output, token_count in zip(outputs, token_counts)
        ]

        return outputs
```

## Conclus√£o

Com todos os componentes definidos, est√° completa a implementa√ß√£o da arquitetura Transformer. No exemplo abaixo est√° uma implementa√ß√£o que os une e executa o modelo, al√©m de um exemplo de uso.

Ainda faltam o treinamento e o algoritmo para usar o modelo autoregressivo.

### Implementa√ß√£o completa em PyTorch

```python
class Transformer(nn.Module):
    def __init__(
        self: Self,
        encoder_config: TransformerLayerConfig,
        decoder_config: TransformerLayerConfig,
        in_vocab: str | set[str],
        out_vocab: str | set[str],
        embed_dim: int = 512,
        theta: int = 10000,
        n_encoder_layers: int = 6,
        n_decoder_layers: int = 6,
    ) -> None:
        super().__init__()

        self.theta = theta
        self.encoder_config = encoder_config
        self.n_encoder_layers = n_encoder_layers
        self.decoder_config = decoder_config
        self.n_decoder_layers = n_decoder_layers
        self.embed_dim = embed_dim

        self.in_vocab = set(in_vocab)
        self.out_vocab = set(out_vocab)

        self.tokenizer = TokenEmbedder(
            embed_dim=self.embed_dim,
            vocab=self.in_vocab,
        )

        self.positional_encoder = PositionalEncoder(
            embed_dim=self.embed_dim,
            theta=self.theta,
        )

        self.input_processor = InputProcessor(
            tokenizer=self.tokenizer,
            positional_encoder=self.positional_encoder,
        )

        self.encoder = Encoder(
            n_layers=self.n_encoder_layers,
            config=self.encoder_config,
        )

        self.decoder = Decoder(
            n_layers=self.n_decoder_layers,
            config=self.decoder_config,
        )

        self.output_processor = OutputProcessor(
            embed_dim=self.embed_dim,
            vocab=self.out_vocab,
        )

    def forward(self: Self, inputs: list[str]) -> str:
        input_tensors, token_counts = self.input_processor(inputs)

        encoder_outputs = self.encoder(
            queries=input_tensors,
            keys=input_tensors,
            values=input_tensors,
        )

        decoder_outputs = self.decoder(
            queries=input_tensors,
            keys=input_tensors,
            values=input_tensors,
            encoder_outputs=encoder_outputs,
        )

        outputs = self.output_processor(decoder_outputs, token_counts)
        return outputs

config = TransformerLayerConfig()

transformer = Transformer(
    in_vocab=printable,
    out_vocab=printable,
    encoder_config=config,
    decoder_config=config,
)

inputs = ["nom", "par", "ca", "da"]

def separate_chars(chars):
    return " ".join(repr(char) for char in chars)

lines = [
    f"{separate_chars(item)} -> {separate_chars(output)}"
    for item, output in zip(inputs, transformer(inputs))
]

print(*lines, sep="\n")
```

## Refer√™ncias

* [Attention is All You Need](https://arxiv.org/abs/1706.03762)
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (ResNet)
* [Layer Normalization](https://arxiv.org/abs/1607.06450)
* [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)

## Recursos adicionais

* [Aula do Andrew Karpathy](https://youtu.be/VMj-3S1tku0?si=TwjBr_x28focppys) sobre conceitos fundamentais de redes neurais e backpropagation.
* [Aula #2 da UvA DL](https://uvadlc-notebooksreadthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html) sobre Redes Neurais Feed-forward em PyTorch
* [Aula #3 da UvA DL](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html) sobre evitar neur√¥nios mortos
* [Aula #4 da UvA DL](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html) sobre t√©cnicas de normaliza√ß√£o da vari√¢ncia

## Agradecimentos

* Ao [Prof. Peter Bloem](https://peterbloem.nl), pelo seu excelente [post](https://peterbloem.nl/blog/transformers) sobre Transformers em detalhe.
