---
title: üáßüá∑ Transformers do zero em PyTorch
description: "Attention is all you need"
date: '2025-01-31'
---

Nesse post, vou explicar como a arquitetura *transformer*, introduzida no artigo *"Attention is All You Need"*, funciona, assumindo o m√≠nimo poss√≠vel de pr√©-requisitos. Al√©m disso, vou escrever uma vers√£o da arquitetura do zero usando *PyTorch*.

Infelizmente, se eu n√£o assumir absolutamente nenhum pr√©-requisito, o conte√∫do ficar√° extenso demais para ser feito de uma vez (embora talvez possa fazer algo parecido caso tenham interesse). Por isso, vou assumir que voc√™ j√° entende:

* Como funcionam opera√ß√µes matriciais (como produto interno, produto matricial, transposi√ß√£o de matrizes, etc.)
* Como funcionam redes neurais *feed-forward*
* Conceitos fundamentais do PyTorch (como tensores, m√≥dulos, *autograd*, otimizadores, etc.)

## Contexto

Transformers s√£o aplica√ß√µes que se destacam em transdu√ß√£o de sequ√™ncias, que s√£o situa√ß√µes onde √© necess√°rio criar um modelo que consegue receber uma sequ√™ncia de elementos de tamanho arbitr√°rio e gerar outra sequ√™ncia de elementos de tamanho arbitr√°rio. Essas duas sequ√™ncias n√£o precisam ter o mesmo comprimento e os elementos das duas sequ√™ncias podem pertencer a conjuntos diferentes.

Embora existam diversas √°reas do conhecimento que possam usar transdu√ß√£o de sequ√™ncias, vamos focar em aplica√ß√µes voltadas √† gera√ß√£o de linguagem natural, ou seja, onde a sequ√™ncia recebida representa um texto e a sequ√™ncia gerada tamb√©m representa um texto.

Embora esses textos possam ser de dom√≠nios diferentes, como em um modelo de tradu√ß√£o entre idiomas, onde os elementos (letras) da sequ√™ncia recebida (texto) pertencem a um vocabul√°rio (o do idioma original), e os elementos (letras) da sequ√™ncia gerada (texto traduzido) pertencem a outro vocabul√°rio (o do idioma alvo). Por√©m, a aplica√ß√£o mais not√°vel dos *transformers* no momento de escrita desse post √© em modelagem de linguagens, onde dada uma sequ√™ncia de elementos (texto), um modelo gera os pr√≥ximos elementos da sequ√™ncia at√© entender que ela chegou ao fim, completando o texto original.

Modelos de linguagem s√£o o que alimentam as intelig√™ncias artificiais conversacionais que tiveram uma explos√£o em popularidade a partir de 2022 com o lan√ßamento de produtos como o ChatGPT, que s√£o baseados em varia√ß√µes da arquitetura *transformer* e mostram o poder desse tipo de aplica√ß√£o e arquitetura quando usados em grande escala.

Muitas das conven√ß√µes que ser√£o descritas tem como foco principal garantir a efici√™ncia dos modelos gerados, tanto do ponto de vista de performance computacional como do ajuste dos modelos, e por ser um campo experimental, e muitas das conclus√µes e consensos tomados na cria√ß√£o de arquiteturas foram tomadas por experi√™ncia emp√≠rica.

## Representa√ß√£o dos dados

Modelos preditivos nada mais s√£o do que algoritmos que podem ser adaptados para conjuntos de dados num√©ricos usando t√©cnicas de otimiza√ß√£o, e *transformers* n√£o s√£o diferentes. Portanto, para conseguir receber e gerar dados textuais, √© necess√°rio criar uma representa√ß√£o num√©rica para esses dados.

### *Tokens*

Como textos s√£o representados por um vocabul√°rio finito e conhecido, √© poss√≠vel enumerar todos os elementos que comp√µem um vocabul√°rio de forma a criar uma fun√ß√£o que associa elementos do vocabul√°rio √†s suas representa√ß√µes num√©ricas. Essas representa√ß√µes num√©ricas s√£o conhecidas como *tokens*, a fun√ß√£o, como *tokenizer*, e no contexto do modelo, o vocabul√°rio √© o conjunto de representa√ß√µes num√©ricas dos elementos do vocabul√°rio original.

Por√©m, ainda √© necess√°rio definir esse vocabul√°rio original. Para definir um vocabul√°rio simples, √© poss√≠vel usar um subconjunto dos caracteres que podem ser representados em um computador moderno, como os caracteres que comp√µem as tabelas *ASCII* e *Unicode*. Por√©m, isso n√£o √© uma restri√ß√£o. Grandes Modelos de Linguagem usados para IAs Generativas e conversacionais frequentemente usam vocabul√°rios compostos de sequ√™ncias de elementos de um subconjunto dos caracteres represent√°veis, gerando fun√ß√µes que podem mapear centenas de milhares de *tokens*, que s√£o escolhidos usando algoritmos especializados para maximizar a performance desses modelos.

| Letra   | Token   |
|---------|---------|
| `a`     | `1`     |
| `b`     | `2`     |
| `c`     | `3`     |
| ...     | ...     |
| `z`     | `26`    |
| `‚ê£`     | `27`    |
| `.`     | `28`    |
| `,`     | `29`    |

Para fins did√°ticos, esse post n√£o vai mostrar como realizar esse tipo de tarefa e vamos usar um vocabul√°rio pequeno composto apenas de um subconjunto dos caracteres que podem ser representados na tabela Unicode. Por√©m, o conceito √© o mesmo, mudando apenas a escala no treinamento do modelo de linguagem.

$$
\begin{array}{cccc}
    \text{g} & \text{a} & \text{t} & \text{o}
\end{array}
\rightarrow
\begin{bmatrix}
    7  &  1  & 20 & 15
\end{bmatrix}
$$

### Processamento em escala

Um grande desafio no uso de redes neurais complexas √© minimzar o tempo de infer√™ncia. √â poss√≠vel implementar um transformer usando apenas estruturas como vari√°veis, listas e la√ßos de repeti√ß√£o em Python puro, por√©m, esse tipo de implementa√ß√£o ing√™nua √© lento demais para ser treinado e usado na pr√°tica pelo grande n√∫mero de computa√ß√µes que precisam ser realizadas na infer√™ncia. Portanto, √© necess√°rio considerar t√©cnicas de programa√ß√£o paralela desde o in√≠cio da implementa√ß√£o do modelo.

Em geral, a forma mais pr√°tica de implementar paralelismo em redes neurais √© aproveitar a capacidade dos dispositivos como placas de v√≠deo de realizar opera√ß√µes matem√°ticas sobre dados tensoriais (como vetores e matrizes) em paralelo de forma muito mais eficiente que processadores. Portanto, mudando a representa√ß√£o dos dados, √© poss√≠vel gerar algoritmos muito mais eficientes.

Uma sequ√™ncia de *tokens* singular gerada a partir de um texto pode ser representanda usando um vetor (tensor unidimensional) com os valores de cada *token* em ordem. Concatenando esses vetores como linhas, √© poss√≠vel representar um lote (*batch*) de textos usando uma matriz (tensor bidimensional), que ser√° a unidade m√≠nima esperada pelo modelo para maximizar a efici√™ncia da computa√ß√£o dos valores.

Mas nesse caso, √© necess√°rio encontrar uma forma de lidar com textos de comprimentos diferentes em um mesmo lote. √â poss√≠vel representar matrizes com linhas de tamanhos diferentes em PyTorch usando o m√≥dulo `torch.nested` (que ainda est√° em fase experimental), mas a forma mais comum de lidar com essa situa√ß√£o √© realizar o preenchimento (*padding*) do tensor. Isso implica em adicionar um *token* especial ao *tokenizer* chamado de *padding token*, que n√£o tem significado real no vocabul√°rio e √© usado apenas para preencher o tensor bidimensional, e adicionar elementos √†s sequ√™ncias com esse valor at√© todas as linhas possu√≠rem o tamanho desejado, que tamb√©m √© arbitr√°rio. As duas formas mais comuns de realizar *padding* s√£o adicionar elementos at√© que todas as linhas at√© que tenham o mesmo tamanho da maior (*max padding*) ou determinar um tamanho m√°ximo fixo (*constant padding*), e essa decis√£o tamb√©m √© um consenso que depende da aplica√ß√£o.

Esse post usar√° o elemento 0 como *pad token* e a t√©cnica de *max padding* para realizar o preenchimento.

| Letra   | Token   |
|---------|---------|
| `<pad>` | `0`     |
| `<bos>` | `1`     |
| `<eos>` | `2`     |
| `a`     | `4`     |
| `b`     | `5`     |
| `c`     | `6`     |
| ...     | ...     |
| `z`     | `28`    |
| `‚ê£`     | `29`    |
| `.`     | `30`    |
| `,`     | `31`    |

<!-- ```txt
+---+---+---+---+                    +----+----+----+----+----+----+----+----+
| g | a | t | o |                    |  7 |  1 | 20 | 15 |  0 |  0 |  0 |  0 |
+---+---+---+---+---+---+---+---+    +----+----+----+----+----+----+----+----+
| e | l | e | f | a | n | t | e |    |  5 | 12 |  5 |  6 |  1 | 14 | 20 |  5 |
+---+---+---+---+---+---+---+---+    +----+----+----+----+----+----+----+----+
| p | e | i | x | e |             -> | 16 |  5 |  9 | 24 |  5 |  0 |  0 |  0 |
+---+---+---+---+---+---+---+        +----+----+----+----+----+----+----+----+
| p | √° | s | s | a | r | o |        | 16 | 27 | 19 | 19 |  1 | 18 | 15 |  0 |
+---+---+---+---+---+---+---+        +----+----+----+----+----+----+----+----+
| c | √£ | o |                        |  3 | 28 | 15 |  0 |  0 |  0 |  0 |  0 |
+---+---+---+                        +----+----+----+----+----+----+----+----+
``` -->

$$
\begin{array}{cccccccc}
    \text{g} & \text{a} & \text{t} & \text{o} & \text{ } & \text{ } & \text{ } & \text{ } \\
    \text{e} & \text{l} & \text{e} & \text{f} & \text{a} & \text{n} & \text{t} & \text{e} \\
    \text{p} & \text{e} & \text{i} & \text{x} & \text{e} & \text{ } & \text{ } & \text{ } \\
    \text{p} & \text{√°} & \text{s} & \text{s} & \text{a} & \text{r} & \text{o} & \text{ } \\
    \text{c} & \text{√£} & \text{o} & \text{ } & \text{ } & \text{ } & \text{ } & \text{ }
\end{array}
\rightarrow
\begin{bmatrix}
    7  &  1  & 20 & 15 &  0 &  0 &  0 &  0 \\
    5  & 12  &  5 &  6 &  1 & 14 & 20 &  5 \\
   16  &  5  &  9 & 24 &  5 &  0 &  0 &  0 \\
   16  & 27  & 19 & 19 &  1 & 18 & 15 &  0 \\
    3  & 28  & 15 &  0 &  0 &  0 &  0 &  0
\end{bmatrix}
$$

Embora essa t√©cnica pare√ßa ser ineficiente, e ela pode ser em certos cen√°rios, como caso o tamanho m√°ximo escolhido seja inadequado, de forma geral, o ganho de performance com a adi√ß√£o desses elementos para preencher a matriz ainda √© superior ao gasto adicional em mem√≥ria, que √© o principal limitante para o treino e infer√™ncia desses modelos do ponto de vista computacional.

Por √∫ltimo, por conven√ß√£o, vamos assumir que os lotes sempre t√™m o mesmo n√∫mero de frases, $b$.

### *Embeddings*

Al√©m da representa√ß√£o num√©rica dos *tokens*, usaremos *token embeddings*, representa√ß√µes vetoriais de cada *token*. Essa representa√ß√£o torna uma matriz de lote de textos em um tensor tridimensional, onde cada elemento (matriz) do tensor representa uma palavra, e cada linha desse elemento √© o *token embedding* de uma palavra.

Portanto, agora a dimens√£o esperada para os dados recebidos √© um tensor de $b$ lotes, cada um com $t$ tokens de dimens√£o $d$.

De forma ing√™nua, √© poss√≠vel criar um vetor esparso (ou seja, possuem grande dimensionalidade e muitos valores como 0) representando o *token*. Essa t√©cnica √© conhecida como *one-hot encoding*, e foi historicamente muito usada em diversas aplica√ß√µes.

<!-- ```txt
                                              1   2   ...   26
                                            +---+---+-----+---+
+---+---+---+---+    +---+---+---+---+    2 | 1 | 0 | ... | 0 |
| b | a | b | a | -> | 2 | 1 | 2 | 1 | -> 1 | 0 | 1 | ... | 0 |
+---+---+---+---+    +---+---+---+---+    2 | 1 | 0 | ... | 0 |
                                          1 | 0 | 1 | ... | 0 |
                                            +---+---+-----+---+
``` -->

$$
\underbrace{
\begin{array}{cccc}
b & a & b & a \\
\end{array}
}_{\text{Original Sequence}}
\quad \to \quad
\underbrace{
\begin{bmatrix}
  2 & 1 & 2 & 1 \\
\end{bmatrix}
}_{\text{Encoded Sequence}}
\quad \to \quad
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & 26 \\
    \hline
    2 & 1 & 0 & \dots & 0 \\
    1 & 0 & 1 & \dots & 0 \\
    2 & 1 & 0 & \dots & 0 \\
    1 & 0 & 1 & \dots & 0
\end{array}
}_{\text{Permutation Matrix}}
$$

Por√©m, os *token embeddings* usados nesse post ser√£o densos (ou seja, possuem dimensionalidade menor em rela√ß√£o a vetores esparsos e poucos valores ser√£o zero) e de tamanho fixo. Esses valores s√£o obtidos usando uma camada linear, que ser√° um par√¢metro trein√°vel da rede neural. √â poss√≠vel imaginar esse par√¢metro como uma matriz onde cada linha representa o valor de um *token embedding* na ordem do vocabul√°rio, assim, multiplicando essa matriz pela representa√ß√£o do *token* usando *one-hot encoding*, √© pos√≠vel obter o valor do *token embedding* diretamente como se fosse um dicion√°rio.

<!-- ```txt
    1   2   ...   26            1     2    ...    d             1     2    ...    d      
  +---+---+-----+---+        +-----+-----+-----+-----+       +-----+-----+-----+-----+
2 | 1 | 0 | ... | 0 |      1 | 0.1 | 0.2 | ... | 0.6 |     2 | 0.7 | 0.8 | ... | 0.3 |
1 | 0 | 1 | ... | 0 |  x   2 | 0.7 | 0.8 | ... | 0.3 |  =  1 | 0.1 | 0.2 | ... | 0.6 |
2 | 1 | 0 | ... | 0 |      3 | 0.4 | 0.5 | ... | 0.9 |     2 | 0.7 | 0.8 | ... | 0.3 |
1 | 0 | 1 | ... | 0 |      4 | 0.9 | 0.1 | ... | 0.5 |     1 | 0.1 | 0.2 | ... | 0.6 |
  +---+---+-----+---+    ... | 0.5 | 0.3 | ... | 0.8 |       +-----+-----+-----+-----+
                          26 | 0.2 | 0.4 | ... | 0.1 |                                                  
                             +-----+-----+-----+-----+                                                  
``` -->

$$
\underbrace{
    \begin{array}{c|cccc}
      & 1 & 2 & \dots & 26 \\
    \hline
    2 & 1 & 0 & \dots & 0 \\
    1 & 0 & 1 & \dots & 0 \\
    2 & 1 & 0 & \dots & 0 \\
    1 & 0 & 1 & \dots & 0
\end{array}
}_{\text{Permutation Matrix}}
\times
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & d \\
    \hline
    1  & 0.1 & 0.2 & \dots & 0.6 \\
    2  & 0.7 & 0.8 & \dots & 0.3 \\
    3  & 0.4 & 0.5 & \dots & 0.9 \\
    4  & 0.9 & 0.1 & \dots & 0.5 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    26 & 0.2 & 0.4 & \dots & 0.1
\end{array}
}_{\text{Data Matrix}}
=
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & d \\
    \hline
    2 & 0.7 & 0.8 & \dots & 0.3 \\
    1 & 0.1 & 0.2 & \dots & 0.6 \\
    2 & 0.7 & 0.8 & \dots & 0.3 \\
    1 & 0.1 & 0.2 & \dots & 0.6
\end{array}
}_{\text{Reordered Data Matrix}}
$$

A maior vantagem de usar essa t√©cnica √© permitir a otimiza√ß√£o dos valores de cada *token embedding* como par√¢metros do modelo, de forma a otimizar a sua performance (veremos mais para a frente como isso √© feito). Al√©m disso, esses vetores possuem tamanho fixo, que frequentemente √© menor que o tamanho do vocabul√°rio, aumentando a velocidade de aprendizado do modelo durante o treinamento. Al√©m disso, veremos que isso pode ser usado para permitir que a arquitetura possa receber frases de qualquer tamanho.

O uso de *embeddings* densos introduz o primeiro hiperpar√¢metro de um *transformer*: a dimens√£o $d$ dos *token embeddings*. No c√≥digo, esse valor ser√° referenciado como `embed_dim`.

```python
class Tokenizer(nn.Module):
    def __init__(self: Self, vocab: str, embed_dim: int) -> None:
        super().__init__()
        self.vocab = set(vocab)
        self.embed_dim = embed_dim
        self.char_ids = {char: index + 1 for index, char in enumerate(self.vocab)}
        self.embedding = nn.Embedding(
            num_embeddings=len(self.vocab) + 1,
            embedding_dim=self.embed_dim,
        )

    def tokenize(self: Self, chars: str) -> list[int]:
        return [self.char_ids[char] for char in chars]

    def forward(self: Self, inputs: list[str]) -> torch.Tensor:
        chars, *inputs = inputs

        tokens = self.tokenize(chars)
        token_lists = [tokens]

        token_count = len(tokens)
        token_counts = [token_count]

        max_length = token_count

        for chars in inputs:
            tokens = self.tokenize(chars)
            token_lists.append(tokens)

            token_count = len(tokens)
            token_counts.append(token_count)

            max_length = max(max_length, token_count)

        for tokens, token_count in zip(token_lists, token_counts):
            tokens.extend(0 for _ in range(max_length - token_count))

        embedding_inputs = torch.tensor(token_lists)
        embeddings = self.embedding(embedding_inputs)
        return embeddings, token_counts
```

## O que √© aten√ß√£o?

No contexto de redes neurais para transdu√ß√£o de sequ√™ncias, aten√ß√£o se refere √† capacidade do modelo de gerar um novo vetor para cada vetor original de forma que os novos vetores sejam mais representativos pelo seu contexto na frase. No caso dos *transformers*, essa combina√ß√£o √© feita a partir de uma m√©dia ponderada de todos os vetores originais, onde o peso de cada vetor √© o valor predito pelo mecanismo de aten√ß√£o.

Imagine a seguinte frase:

> Ent√£o, Jo√£o se lembrou: O √¥nibus estava cheio, mas ele conseguiu uma cadeira livre.

Considere que cada palavra representa um *token*. Nesse caso, o mecanismo de aten√ß√£o permite alterar o valor do *token embedding* do *token* "ele" para seja mais pr√≥ximo do valor de "Jo√£o" do que de "√¥nibus", preservando o contexto da frase no valor de cada *embedding* individual. Isso √© importante porque permite que as pr√≥ximas possam fazer transforma√ß√µes nos dados sem ter que levar todo o texto em considera√ß√£o, que de forma geral significa poder fazer predi√ß√µes que levam o contexto do texto em considera√ß√£o de forma mais r√°pida que as alternativas.

A aten√ß√£o total que pode ser prestada entre os *embeddings* √© finita, ou seja, √© descrita como um vetor de valores entre 0 e 1 cuja soma √© 1. Logo, se um *embedding* recebe aten√ß√£o a mais, os outros receber√£o a menos de alguma forma.

## *Queries, Keys, Values*

No contexto dos mecanismos de aten√ß√£o, os *token embeddings* possuem nomes diferentes dependendo do seu papel. Fazendo uma analogia com um dicion√°rio em Python, onde dado um conjunto de chaves que s√£o associadas a outros valores, √© poss√≠vel encontrar o valor associado ao valor de uma vari√°vel de consulta no dicion√°rio comparando essa vari√°vel a partir das chaves e retornando o valor dessa chave. Um mecanismo de aten√ß√£o √© similar, por√©m ao inv√©s de retornar apenas um valor associado a uma chave, dado uma vari√°vel de consulta, √© obtido uma fra√ß√£o de cada valor no dicion√°rio a partir da compara√ß√£o com as chaves.

Essa analogia tem mais valor hist√≥rico que figurativo para explicar o funcionamento de um mecanismo de aten√ß√£o, mas gera a nomenclatura original para as vari√°veis envolvidas. Para um conjunto de *embeddings* representando uma sequ√™ncia, o *embedding* que ter√° seu novo valor gerado √© a consulta (*query*), os *embeddings* que ser√£o comparados com a *query* para gerar a aten√ß√£o ser√£o as chaves do dicion√°rio (*keys*), e os valores que ser√£o multiplicados pela aten√ß√£o ser√£o os valores do dicion√°rio (*values*).

A analogia acima usa apenas um *embedding* por vez, mas como falamos anteriormente, √© importante realizar o m√°ximo de opera√ß√µes em lote que forem poss√≠veis para maximizar a performance do modelo. Ent√£o, visualizando de forma matricial, se para cada um dos $b$ lotes recebidos, os *tokens* de um texto s√£o representados como uma matriz de $t$ *embeddings* de dimens√£o $d$, a aten√ß√£o em um *transformer* ser√° representada como um tensor com $b$ lotes de matrizes $t \times t$, onde cada linha representa a aten√ß√£o que um *embedding* deve considerar nos *values* gerados:

<!-- ```txt

       1     2    ...    d             1     2     ...     t              1     2    ...    d   
    +-----+-----+-----+-----+       +-----+-----+-------+-----+        +-----+-----+-----+-----+
  1 | 0.7 | 0.8 | ... | 0.3 |     1 | 0.5 | 0.5 |  ...  | 0.0 |      1 | 0.4 | 0.3 | ... | 0.8 |
  2 | 0.3 | 0.2 | ... | 0.2 |  x  2 | 0.2 | 0.3 |  ...  | 0.4 | ->   2 | 0.9 | 0.1 | ... | 0.1 |
... | 0.1 | 0.9 | ... | 0.5 |   ... | ... | ... |  ...  | ... |    ... | 0.8 | 0.7 | ... | 0.4 |
  t | 0.2 | 0.4 | ... | 0.1 |     t | 0.1 | 0.2 |  ...  | 0.5 |      t | 0.2 | 0.5 | ... | 0.3 |
    +-----+-----+-----+-----+       +-----+-----+-------+-----+        +-----+-----+-----+-----+
``` -->

$$
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & d \\
    \hline
    1  & 0.7 & 0.8 & \dots & 0.3 \\
    2  & 0.3 & 0.2 & \dots & 0.2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    t  & 0.2 & 0.4 & \dots & 0.1
\end{array}
}_{\text{Feature Matrix}}
\times
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & t \\
    \hline
    1  & 0.5 & 0.5 & \dots & 0.0 \\
    2  & 0.2 & 0.3 & \dots & 0.4 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    t  & 0.1 & 0.2 & \dots & 0.5
\end{array}
}_{\text{Transformation Matrix}}
\rightarrow
\underbrace{
\begin{array}{c|cccc}
      & 1 & 2 & \dots & d \\
    \hline
    1  & 0.4 & 0.3 & \dots & 0.8 \\
    2  & 0.9 & 0.1 & \dots & 0.1 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    t  & 0.2 & 0.5 & \dots & 0.3
\end{array}
}_{\text{Transformed Feature Matrix}}
$$

Note que dessa forma √© poss√≠vel calcular os valores novos de todos os *embeddings* simultaneamente.

A forma como a aten√ß√£o √© calculada para cada *token* √© determinada pelo mecanismo de aten√ß√£o. O mecanismo usado originalmente nos *transformers* √© conhecido como *Multihead Attention*, que √© uma varia√ß√£o do *Scaled Dot-Product Attention*, que √© uma varia√ß√£o do *Dot-Product Attention* que √© um tipo de *Self-Attention*.

*Self-Attention* √© um termo que tem mais valor hist√≥rico que pr√°tico atualmente, mas significa que os vetores de aten√ß√£o n√£o s√£o gerados por nenhum modelo param√©trico externo ao *transformer*: Os *embeddings* originais s√£o usados tanto para determinar quanta aten√ß√£o deve ser prestada em cada *embedding* no valor final quanto para gerar os valores dos novos *embeddings*.

## *Scaled Dot-Product Attention*

No caso do mecanismo de *Dot-Product Attention*, a aten√ß√£o prestada √© feita a partir do produto interno (*dot product*) dos *token embeddings*. O modelo toma a premissa de que cada *embedding* deve prestar aten√ß√£o em outro *embedding* de forma diretamente proporcional ao produto interno deles. Pensando de forma matricial, a aten√ß√£o pode ser calculada como:

$$
  \text{DPA}(Q,K,V) = Q^TKV
$$

Note que dessa forma √© poss√≠vel calcular a aten√ß√£o de todos os *embeddings* entre si e os seus valores novos simultaneamente.

Por√©m, existe um problema nesse processo. Embora a aten√ß√£o entre cada par de *embeddings* esteja sempre entre 0 e 1, o produto interno de dois vetores est√° entre $-\infty$ e $\infty$, o que permite o mecanismo prestar aten√ß√£o infinita entre todos os valores, o que pode atrapalhar o treinamento do modelo. Para normalizar os *embeddings*, a aten√ß√£o √© normalizada usando a fun√ß√£o *softmax*, que faz com que os valores de cada linha sejam normalizados entre 0 e 1 preservando a propor√ß√£o dos valores originais.

$$
  \text{DPA}(Q,K,V) = \text{Softmax}(Q^TK)V
$$

Na implementa√ß√£o original dos *transformers*, o mecanismo usado √© uma varia√ß√£o do *Scaled Dot-Product Attention*, que √© diferente do *Dot-Product Attention* em apenas uma coisa:

$$
  \text{SDPA}(Q,K,V) = \text{Softmax}\left(\frac{Q^TK}{\sqrt{d_{in}}}\right)V
$$

Onde $d$ √© a dimens√£o dos *embeddings*. Essa normaliza√ß√£o dos valores tem origem emp√≠rica, onde foi observado que realizar processos de normaliza√ß√£o como esses evita que redes neurais sejam treinadas incorretamente por causa da magnitude dos elementos da matriz de aten√ß√£o antes da fun√ß√£o *softmax*.

## Proje√ß√µes lineares

Um dos segredos do *transformer* para tornar o mecanismo de aten√ß√£o mais eficiente √© projetar linearmente as *Queries*, *Keys* e *Values*. Isso significa multiplicar cada um desses valores por uma matriz de par√¢metros trein√°veis (denominadas $W^Q$, $W^K$ e $W^V$). Isso faz com que os valores das *Queries*, *Keys* e *Values* sejam diferentes entre si, e durante o treino, os valores dessas matrizes de par√¢metros sejam otimizados para maximizar como o *transformer* converte os *embeddings* originais em novos *embeddings* que representam melhor o valor de cada *token* no contexto onde est√£o inseridos.

$$
  \text{SDPA-Transformer}(Q,K,V) =  SDPA(QW^Q, KW^K,VW^V)
$$

## *Multihead Attention*

O verdadeiro mecanismo utilizado originalmente nos *transformers √© conhecido como *Multihead Attention*, que consiste em aplicar *Scaled Dot-Product Attention* $h$ vezes sobre os *embeddings* para combinar os resultados dessas aplica√ß√µes. O valor $h$ (ou o n√∫mero de cabe√ßas) √© um hiperpar√¢metro do modelo.

Como n√£o √© poss√≠vel prestar aten√ß√£o em todos os elementos simultaneamente, o objetivo desse mecanismo √© permitir que um *embedding* possa ser gerado prestando aten√ß√£o em partes diferentes dos outros valores ao mesmo tempo. √â como se o modelo tivesse v√°rios pares de olhos independentes e conseguisse ler v√°rias partes de um texto ao mesmo tempo, e ainda conseguisse entender o que est√° acontecendo.

O maior problema de usar esse mecanismo da forma descrita acima √© que a aten√ß√£o precisa ser calculada $h$ vezes, ent√£o se torna $h$ vezes mais lenta, e um dos objetivos dos *transformers* √© ter modelos que podem fazer predi√ß√µes rapidamente. Por isso, √© usado um truque para manter o tempo desse algoritmo constante em rela√ß√£o ao n√∫mero de cabe√ßas:

1. Dividir as *Queries*, *Keys* e *Values* em $h$ peda√ßos individuais (ou aplicar $h$ proje√ß√µes lineares diferentes nas *Queries*, *Keys* e *Values*), fazendo com que as dimens√µes do tensor passem de $b \times t \times d_{in}$ para $b \times t \times h \times \frac{h}{d_{in}}$.
2. Reordenar a ordem dos elementos das *Queries*, *Keys* e *Values*, fazendo com que as dimens√µes do tensor passem de $b \times t \times h \times \frac{h}{d_{in}}$ para $b \times h \times t \times \frac{h}{d_{in}}$.
3. Aplicar *Scaled Dot Product-Attention* nos *embeddings* recebidos, usando proje√ß√µes diferentes para cada cabe√ßa.
4. Concatenar os *embeddings* de cada cabe√ßa e retornar a ordem original dos elementos, fazendo com que o tensor volte a ter dimens√µes $b \times t \times d_{in}$.
5. Aplicar uma proje√ß√£o linear $W^O$ nos *embeddings* concatenados.

$$
  MHA(Q,K,V) = \left(\Big \Vert^h_{i=1} SDPA(QW^Q_i, KW^K_i,VW^V_i) \right) W^O
$$

Dessa forma, apesar da aten√ß√£o precisar ser calculada $h$ vezes, cada c√°lculo √© $\frac{1}{h}$ vezes mais r√°pido que o original, o que leva o mesmo tempo que aplicar *Scaled Dot-Product Attention*. No final, aplicar *Multihead Attention* ainda vai ser mais lento que aplicar *Scaled Dot-Product Attention*, porque o novo mecanismo ainda precisa realizar a proje√ß√£o linear $W^O$, mas o tempo adicional n√£o aumenta com rela√ß√£o a $h$. Essa diferen√ßa √© ainda maior se uma proje√ß√£o linear a base de multiplica√ß√µes de matrizes for feita para cada cabe√ßa, caso em que a diferen√ßa aumenta com rela√ß√£o a $h$. Por√©m, se a proje√ß√£o for feita dividindo os valores originais manipulando os elementos da matriz (como veremos no c√≥digo em PyTorch), essa etapa tem tempo constante e pode ser omitida.

```python
class MultiheadAttention(nn.Module):
    def __init__(self: Self, embed_dim: int, n_heads: int) -> None:
        super().__init__()

        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.head_dim = self.embed_dim // self.n_heads

        self.query_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.key_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.value_projection = nn.Linear(embed_dim, embed_dim, bias=False)
        self.output_projection = nn.Linear(embed_dim, embed_dim, bias=False)

    def split_embeddings(
        self: Self,
        inputs: torch.Tensor,
        batches: int,
        tokens: int,
    ) -> torch.Tensor:
        split_inputs = inputs.view(batches, tokens, self.n_heads, self.head_dim)
        head_sorted_inputs = split_inputs.transpose(1, 2)
        return head_sorted_inputs

    def join_embeddings(
        self: Self,
        inputs: torch.Tensor,
        batches: int,
        tokens: int,
    ) -> torch.Tensor:
        token_sorted_inputs = inputs.transpose(1, 2)
        token_sorted_inputs = token_sorted_inputs.contiguous()
        joined_inputs = token_sorted_inputs.view(batches, tokens, self.embed_dim)
        return joined_inputs

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        batches, tokens, _ = queries.size()

        queries = self.query_projection(queries)
        keys = self.key_projection(keys)
        values = self.value_projection(values)

        queries = self.split_embeddings(queries, batches, tokens)
        keys = self.split_embeddings(keys, batches, tokens)
        values = self.split_embeddings(values, batches, tokens)

        keys = keys.transpose(2, 3)

        scores = queries @ keys / (self.head_dim**0.5)

        if mask is not None:
            scores = scores.masked_fill(mask, float("-inf"))
            print(scores)

        weights = F.softmax(scores, dim=3)

        attn_outputs = weights @ values
        joined_outputs = self.join_embeddings(attn_outputs, batches, tokens)
        projected_outputs = self.output_projection(joined_outputs)

        return projected_outputs
```

## *Positional encoding*

Uma falha na defini√ß√£o dos mecanismos de aten√ß√£o da forma com que foram mostrados √© que nenhuma das opera√ß√µes envolvidas nos mecanismos leva em considera√ß√£o a posi√ß√£o dos elementos na sequ√™ncia para a cria√ß√£o dos novos elementos gerados pelo mecanismo. Isso significa que permuta√ß√µes na ordem dos elementos n√£o afetam o resultado, o que √© um efeito indesejado e que pode atrapalhar o processo de treino do modelo. Durante o treinamento, o *transformer* deve aprender a projetar os elementos originais de forma que preserve o seu contexto na frase, mas se a posi√ß√£o n√£o for levada em considera√ß√£o, √© dif√≠cil que o modelo consiga remover ambiguidades de forma efetiva.

Para mitigar esse problema, antes da aplica√ß√£o de qualquer mecanismo de aten√ß√£o, os elementos originais passam por um processo de codifica√ß√£o posicional (*Posicional Encoding*), que tem como objetivo alterar os valores de cada elemento para representar sua posi√ß√£o na sequ√™ncia original. Como essa codifica√ß√£o altera os valores dos elementos, alterar√° seus produtos internos e por consequ√™ncia, a aten√ß√£o prestada pelo modelo em cada elemento.

A forma com que essa codifica√ß√£o √© realizada na implementa√ß√£o original segue a fun√ß√£o a seguir:

$$
    \text{PE}(i, j, p) =
    \begin{cases}
    \sin \dfrac{p}{\theta^{\frac{2i}{d_{in}}}} & \text{se }j \text{ √© par}, \\ ~ \\
    \cos \dfrac{p}{\theta^{\frac{2i}{d_{in}}}} & \text{se }j \text{ √© √≠mpar}.
    \end{cases}
$$

Onde:

* $p$ √© a posi√ß√£o de um elemento na sequ√™ncia (ou linha da matriz atual).
* $j$ √© a posi√ß√£o de um item em um elemento da sequ√™ncia (ou coluna da linha atual)
* $0 \leq i < \frac{d}{2}$, e seu valor aumenta em um a cada dois itens consecutivos de um elemento da sequ√™ncia.
* $\theta$ √© um hiperpar√¢metro.

Embora a explica√ß√£o a seguir envolva conceitos externos a esse post, essa fun√ß√£o √© usada porque √© o equivalente a aplicar uma matriz de rota√ß√£o nos elementos da sequ√™ncia, onde o √¢ngulo de rota√ß√£o de um elemento √© determinado pela sua posi√ß√£o. Isso significa que a codifica√ß√£o posicional √© relativa, ou seja, prioriza representar a posi√ß√£o de um elemento em rela√ß√£o aos seus vizinhos mais do que representar a ordem dos elementos de forma absoluta.

De forma matricial, os valores de $PE$ para uma sequ√™ncia com 5 elementos com $d = 4$ e $\theta = 10000$  podem ser visualizados da seguinte forma:

$$
i =
\begin{bmatrix}
0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}
$$
$$
j =
\begin{bmatrix}
0 & 1 & 2 & 3 \\
0 & 1 & 2 & 3 \\
0 & 1 & 2 & 3 \\
0 & 1 & 2 & 3 \\
0 & 1 & 2 & 3 \\
\end{bmatrix}
$$
$$
p =
\begin{bmatrix}
0 & 0 & 0 & 0 \\
1 & 1 & 1 & 1 \\
2 & 2 & 2 & 2 \\
3 & 3 & 3 & 3 \\
4 & 4 & 4 & 4 \\
\end{bmatrix}
$$

$$
PE(i,j,p) =
\begin{bmatrix}
0.0000 & 1.0000 & 0.0000 & 1.0000 \\
0.8415 & 0.5403 & 0.0100 & 0.9999 \\
0.9093 & -0.4161 & 0.0200 & 0.9998 \\
0.1411 & -0.9899 & 0.0300 & 0.9996 \\
-0.7568 & -0.6536 & 0.0400 & 0.9992 \\
\end{bmatrix}
$$

Por fim, $PE(i,j,p)$ √© somado ao lote de sequ√™ncias, e esse resultado ser√° o valor usado pelo modelo como entrada.

```python
class PositionalEncoder(nn.Module):
    def __init__(self: Self, embed_dim: int, theta: int) -> None:
        super().__init__()
        self.embed_dim = embed_dim
        self.theta = theta

    @torch.no_grad()
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        batches, tokens, _ = inputs.size()

        indexes = torch.arange(self.embed_dim, dtype=torch.float)

        positions = torch.arange(tokens, dtype=torch.float)
        positions = positions.view(tokens, 1)

        i = torch.arange(self.embed_dim // 2)
        i = i.float()
        i = i.repeat_interleave(2)

        cos_indexes = indexes % 2
        cos_indexes = cos_indexes.bool()
        cos_indexes = cos_indexes.expand((tokens, self.embed_dim))

        sin_indexes = ~cos_indexes

        encodings = positions / (self.theta ** (2 * i / self.embed_dim))

        encodings[sin_indexes] = encodings[sin_indexes].sin()
        encodings[cos_indexes] = encodings[cos_indexes].cos()

        encodings = encodings.expand((batches, tokens, self.embed_dim))

        return inputs + encodings
```

## Transdu√ß√£o autoregressiva de sequ√™ncias

Antes de falar de como um *transformer* funciona, √© importante entender como podemos fazer transdu√ß√£o de sequ√™ncias a partir de outra sequ√™ncia de elementos de forma geral para simplificar a explica√ß√£o do funcionamento. Al√©m disso, √© importante entender o que significa fazer essa tarefa de forma autoregressiva, caracter√≠stica desse tipo de modelo.

Vamos voltar para o caso em que estamos criando um modelo que recebe um texto e gera outro. Como vimos anteriormente, o modelo recebe $b$ sequ√™ncias de $t$ elementos, representando os textos recebidos. Do ponto de vista de uma sequ√™ncia, o resultado √© outra sequ√™ncia de $t$ elementos, que representa a mesma sequ√™ncia recebida sem o primeiro *token* e com um novo *token*, decidido pelo modelo, ao final. O processo de remover o primeiro *token* da sequ√™ncia recebida e adicionar o pr√≥ximo *token* da sequ√™ncia esperada ser√° denominado *shift*.

Esse processo √© o equivalente a gerar o pr√≥ximo *token* do texto recebido. Se essa sequ√™ncia gerada for usada como a sequ√™ncia recebida do modelo novamente, ser√° gerado mais um novo *token*, e o processo √© repetido at√© que o modelo gere o *token* especial de fim de sequ√™ncia. esse processo √© o que torna o modelo autoregressivo, ou seja, as predi√ß√µes do modelo tamb√©m s√£o usadas como seus dados recebidos at√© que o modelo chegue em certo crit√©rio de parada, que no caso √© o *token* especial.

O exemplo a seguir ilustra como um modelo faria a tradu√ß√£o da palavra "cachorro" em Portugu√™s para a palavra "dog" em Ingl√™s. As colunas representam os valores das sequ√™ncias em cada posi√ß√£o, e as linhas, as itera√ß√µes do algoritmo:

$$
\underbrace{
\begin{array}{c|cccccccccc}
    & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\hline
 1 & \text{<bos>} & \text{c} & \text{a} & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} \\
 2 & \text{c} & \text{a} & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} \\
 3 & \text{a} & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} \\
 4 & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} & \text{o} \\
 5 & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} & \text{o} & \text{g} \\
\end{array}
}_{\text{Sequ√™ncia original}}
$$

$$
\rightarrow
$$

$$
\underbrace{
\begin{array}{c|cccccccccc}
    & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\hline
 1 & \text{c} & \text{a} & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} \\
 2 & \text{a} & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} \\
 3 & \text{c} & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} & \text{o} \\
 4 & \text{h} & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} & \text{o} & \text{g} \\
 5 & \text{o} & \text{r} & \text{r} & \text{o} & \text{<eos>} & \text{<bos>} & \text{d} & \text{o} & \text{g} & \text{<eos>} \\
\end{array}
}_{\text{Sequ√™ncia ap√≥s o \textit{shift}}}
$$

Note que o primeiro *shift* sempre ter√° o mesmo resultado: remover o *token* `<bos>` e o adicionar ao final da sequ√™ncia, porque isso ser√° importante na implementa√ß√£o da arquitetura. Al√©m disso, como a concatena√ß√£o dos *tokens* gerados dessa forma a cada itera√ß√£o representam a sequ√™ncia gerada, o tamanho dessa sequ√™ncia √© indepentedente do tamanho da sequ√™ncia recebida, logo √© poss√≠vel gerar qualquer sequ√™ncia de tamanho arbitr√°rio.

### M√°scara de aten√ß√£o

A descri√ß√£o do algoritmo acima √© bem mais antiga que os *transformers*, e por mais que um algoritmo fa√ßa sentido teoricamente ao assumir que o modelo ir√° acertar o processo, ainda √© necess√°rio criar um modelo capaz de realizar a predi√ß√£o no formato proposto corretamente. E no caso de transdu√ß√£o de sequ√™ncias, √© poss√≠vel argumentar que modelos que consigam predizer elementos da forma correta ainda √© um desafio em diversas aplica√ß√µes, mesmo com arquiteturas modernas como as usadas nos *Large Language Models*.

Um problema encontrado em outras arquiteturas para a mesma tarefa √© garantir converg√™ncia para o m√≠nimo global durante o treinamento. Por exemplo, arquiteturas baseadas em redes neurais recorrentes (*RNNs*) podem ter problemas no treinamento envolvendo desaparecimento de gradientes (*gradient vanishing*), efeito que acontece quando os valores dos gradientes gerados pelo processo de *backpropagation* s√£o pequenos demais para que o modelo consiga convergir at√© o final de treinamento, fazendo com que o treinamento praticamente pare antes do valor da fun√ß√£o de perda se tornar pr√≥ximo do m√≠nimo global. J√° outras arquiteturas baseadas em *sequence-to-sequence learning* resolvem esse problema, mas podem se tornar muito lentas durante a infer√™ncia pela sua natureza sequencial.

Um dos objetivos de realizar opera√ß√µes em paralelo sobre todos os elementos da sequ√™ncia nos mecanismos de aten√ß√£o estudados √© evitar esses dois problemas simultaneamente, e os autores mostram que √© poss√≠vel realizar isso usando basicamente apenas mecanismos de aten√ß√£o, ou seja, outros mecanismos n√£o s√£o necess√°rios, **aten√ß√£o √© tudo que voc√™ precisa (*Attention is all you need*)**.

Mas, durante o treinamento desses modelos, para que o modelo consiga aprender a predizer o pr√≥ximo elemento da sequ√™ncia correntamente, √© necess√°rio limitar o seu conhecimento sobre os valores da sequ√™ncia para que a sequ√™ncia gerada pelo mecanismo de aten√ß√£o seja gerada corretamente.

Perceba que os mecanismos de aten√ß√£o mostrados at√© aqui consideram todos os elementos de uma sequ√™ncia na gera√ß√£o dos novos elementos o que significa que o i-√©simo elemento da sequ√™ncia gerada considerar√° o i+1-√©simo elemento, e tamb√©m o i+2-√©simo e assim por diante. A consequ√™ncia disso √© que isso afetar√° o processo de *backpropagation*, j√° que como durante o treinamento, a perda ser√° calculada a partir do resultado de cada itera√ß√£o do algoritmo de autoregress√£o mostrado acima (que ser√° a mesma sequ√™ncia, mas sem o primeiro elemento recebido e com um novo elemento gerado por √∫ltimo).

Considere agora como a perda ser√° calculada nesse caso. Para todos os elementos exceto o √∫ltimo, os elementos da sequ√™ncia gerada pelo mecanismo de aten√ß√£o j√° estavam na sequ√™ncia original, e foram apenas deslocados uma posi√ß√£o √† esquerda. Portanto, quando os par√¢metros trein√°veis do mecanismo de aten√ß√£o forem atualizados, eles convergir√£o de forma que ir√° perceber esse processo e apenas deslocar os valores dos elementos para a esquerda em uma posi√ß√£o, o que pode facilmente inviabilizar a converg√™ncia do modelo durante o treinamento.

Portanto, √© necess√°rio usar uma forma de limitar o conhecimento do modelo durante a aten√ß√£o para que o i-√©simo elemento da sequ√™ncia gerada n√£o seja escolhido considerando o valor dos pr√≥ximos valores na sequ√™ncia original, mesmo que esse seja o objetivo do modelo durante o treinamento. A forma com que isso √© feito √© aplicando uma m√°scara de aten√ß√£o (*attention mask*), que anula os valores posteriores a um elemento na sequ√™ncia para cada elemento no tensor de aten√ß√£o.

A forma com que implementaremos a aplica√ß√£o da matriz de aten√ß√£o √© transformar esses valores em $-\infty$ antes de aplicar a fun√ß√£o *softmax* somando um tensor onde cada elemento √© uma matriz triangular superior da seguinte forma:

$$
  \text{SDPA-Mask}(Q,K,V,M) = \text{Softmax}\left(\frac{Q^TK}{\sqrt{d_{in}}}+M\right)V
$$

$$
\underbrace{
\begin{array}{c|ccccc}
       & 1      & 2      & 3      & \dots  & t-1    & t      \\
\hline
1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
\end{array}
}_{\text{Aten√ß√£o original}}
$$
$$
+
$$
$$
\underbrace{
\begin{array}{c|ccccc}
    & 1      & 2          & 3        & \dots  & t-1      & t        \\
\hline
1        & 0      & -\infty  & -\infty  & \dots  & -\infty  & -\infty  \\
2        & 0      & 0        & -\infty  & \dots  & -\infty  & -\infty  \\
3        & 0      & 0        & 0        & \dots  & -\infty  & -\infty  \\
\vdots   & \vdots & \vdots   & \vdots   & \ddots & \vdots   & \vdots   \\
t-1      & 0      & 0        & 0        & \dots  & 0        & -\infty  \\
t        & 0      & 0        & 0        & \dots  & 0        & 0
\end{array}
}_{\text{M√°scara de aten√ß√£o}}
$$
$$
=
$$
$$
\underbrace{
\begin{array}{c|ccccc}
    & 1      & 2      & 3      & \dots  & t-1    & t      \\
\hline
1      & 6.32   & -\infty & -\infty    & \dots  & -\infty  & -\infty  \\
2      & -1.81  & 0.96    & -\infty    & \dots  & -\infty  & -\infty  \\
3      & -1.81  & 0.06    & -2.27      & \dots  & -\infty  & -\infty  \\
\vdots & \vdots & \vdots  & \vdots     & \ddots & \vdots   & \vdots   \\
t-1    & -0.11  & 1.55    & -0.18      & \dots  & 0.95     & -\infty  \\
t      & 1.45   & -1.42   & 1.62       & \dots  & 2.06     & -0.23
\end{array}
}_{\text{Aten√ß√£o mascarada}}
$$

A m√°scara √© aplicada antes da fun√ß√£o *softmax* porque como $-\infty$ √© o menor valor real poss√≠vel, esses elementos se tornar√£o zero ap√≥s a normaliza√ß√£o, enquanto que a soma dos demais elementos ser√° igual a 1, fazendo com que a aten√ß√£o ignore completamente esses elementos.

Veremos ainda que apesar dessa limita√ß√£o criada para garantir funcionamento do modelo durante o algoritmo de autoregress√£o, existem casos na arquitetura onde n√£o h√° problemas em n√£o usar uma m√°scara de aten√ß√£o, e nesse caso, a m√°scara usada ser√° apenas um tensor nulo, assim, a aplica√ß√£o da m√°scara nula n√£o surtir√° nenhum efeito no resultado final.

$$
\underbrace{
\begin{array}{c|ccccc}
       & 1      & 2      & 3      & \dots  & t-1    & t      \\
\hline
1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
\end{array}
}_{\text{Aten√ß√£o original}}
$$
$$
+
$$
$$
\underbrace{
\begin{array}{c|ccccc}
    & 1      & 2          & 3        & \dots  & t-1      & t        \\
\hline
1        & 0      & 0        & 0        & \dots  & 0        & 0        \\
2        & 0      & 0        & 0        & \dots  & 0        & 0        \\
3        & 0      & 0        & 0        & \dots  & 0        & 0        \\
\vdots   & \vdots & \vdots   & \vdots   & \ddots & \vdots   & \vdots   \\
t-1      & 0      & 0        & 0        & \dots  & 0        & 0        \\
t        & 0      & 0        & 0        & \dots  & 0        & 0
\end{array}
}_{\text{M√°scara de aten√ß√£o}}
$$
$$
=
$$
$$
\underbrace{
\begin{array}{c|ccccc}
       & 1      & 2      & 3      & \dots  & t-1    & t      \\
\hline
1      & 6.32   & -1.12  & 1.32   & \dots  & -2.16  & 1.92   \\
2      & -1.81  & 0.96   & 0.89   & \dots  & 0.03   & 1.76   \\
3      & -1.81  & 0.06   & -2.27  & \dots  & 1.01   & -1.32  \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
t-1    & -0.11  & 1.55   & -0.18  & \dots  & 0.95   & 0.95   \\
t      & 1.45   & -1.42  & 1.62   & \dots  & 2.06  & -0.23
\end{array}
}_{\text{Aten√ß√£o original}}
$$

```python
def get_attn_mask(size: int | tuple[int]) -> torch.Tensor:
    mask = torch.ones(size)
    mask = mask.triu(diagonal=1)
    mask = mask.bool()
    return mask
```

## Montando um *transformer*

```mermaid
flowchart
    input(Sequ√™ncia recebida) --> inputProcessing(Processamento de entrada)
    inputProcessing --> encoder("`Codificador (*Encoder*)`") --> decoder("`Decodificador (*Decoder*)`") --> outputProcessing(Processamento de sa√≠da)
    inputProcessing --> decoder
    outputProcessing --> output(Sequ√™ncia gerada)
```

Agora que os conceitos principais da arquitetura foram apresentados, podemos visualizar como o modelo funciona.

Os *transformers* originalmente s√£o compostos de opera√ß√µes similares que s√£o aplicadas de forma sequencial sobre um lote de sequ√™ncias, uma escolha que, como a maioria delas at√© aqui, tem como objetivo aumentar a efici√™ncia computacional do modelo durante treino e infer√™ncia. √â poss√≠vel visualizar um *transformer* como sendo composto dos seguintes componentes:

### Processamento de entrada

```mermaid
flowchart
    encoder(*Encoder*)
    input(Sequ√™ncia recebida)
    tokenizing(*Tokenizing*)
    shift(*Shifting*)
    pose("*Positional Encoding*")
    embedding("*Embedding*")
    decoder(*Decoder*)
    input --> tokenizing --> embedding --> pose -- Sequ√™ncia recebida --> encoder
    tokenizing --> shift --> embedding
    pose --  Sequ√™ncia com *shift* --> decoder
```

Todos os componentes do processamento de entrada j√° foram introduzidos antes:

1. Uma sequ√™ncia tem seus elementos convertidos para *tokens*
2. Os *tokens* s√£o convertidos em *token embeddings*
3. *Positional Encoding* √© aplicado sobre os *token embeddings*, gerando o lote de sequ√™ncias que ser√° usado pelo modelo como entrada de fato.

```python
class InputProcessor(nn.Module):
    def __init__(
        self: Self,
        tokenizer: Tokenizer,
        positional_encoder: PositionalEncoder,
    ) -> None:
        super().__init__()
        self.tokenizer = tokenizer
        self.positional_encoder = positional_encoder

    def forward(self: Self, inputs: list[str]) -> torch.Tensor:
        input_tensors, token_counts = self.tokenizer(inputs)
        encoded_tensors = self.positional_encoder(input_tensors)
        return encoded_tensors, token_counts
```

### Camada de *Transformer*

```mermaid
flowchart
    Key(*Keys*)
    Mask(M√°scara nula)    
    Query(*Queries*)
    Value(*Values*)
    Sum1(Soma)
    Sum2(Soma)
    MHA(*Multihead Attention*)
    LayerNorm1(*LayerNorm*)
    LayerNorm2(*LayerNorm*)
    Linear1(Camada linear)
    Linear2(Camada linear)
    ReLU(ReLU)
    output(Sequ√™ncia gerada)
    Mask --> MHA
    Query --> MHA
    Key --> MHA
    Value --> MHA
    MHA --> Sum1
    Sum1 --> LayerNorm1
    LayerNorm1 --> Linear1
    Linear1 --> ReLU
    ReLU --> Linear2
    LayerNorm1 --> Sum2
    Linear2 --> Sum2
    Sum2 --> LayerNorm2
    LayerNorm2 --> output
    Value --> Sum1
```

Antes de falar sobre o *encoder* e *decoder*, vamos ver as camadas de *transformer*, que s√£o uma opera√ß√£o que acontece em ambos os componentes. Essas camadas operam da seguinte forma:

1. As *Queries*, *Keys* e *Values* passam por um mecanismo de *Multihead Attention*, gerando uma sequ√™ncia de elementos com valores atualizados para representar melhor os seus contextos na sequ√™ncia.
2. A sequ√™ncia gerada √© somada aos *Values* originais, em uma opera√ß√£o denominada conex√£o residual.
3. A sequ√™ncia resultante tem seus valores normalizados atrav√©s de uma opera√ß√£o denominada *LayerNorm*.
4. A sequ√™ncia normalizada passa por uma rede neural *feed-forward* composta por:
    1. Uma camada linear que aumenta a dimens√£o dos elementos da sequ√™ncia para $d_{ff}$.
    2. Uma fun√ß√£o de ativa√ß√£o ReLU.
    3. Outra camada linear que reduz a dimens√£o dos elementos da sequ√™ncia de volta para $d_{in}$.
5. A sequ√™ncia gerada √© somada ao resultado do *LayerNorm* anterior em uma nova conex√£o residual.
6. A sequ√™ncia resultante tem seus valores normalizados atrav√©s de uma nova *LayerNorm*.

Conex√µes residuais s√£o muito comuns em redes neurais desde a sua proposi√ß√£o na arquitetura *ResNet*, que demostrou empiricamente que essa opera√ß√£o torna a fun√ß√£o de custo mais suave (ou seja, com menos m√≠nimos locais) e faz com que o custo do modelo se aproxime do m√≠nimo global mais rapidamente durante o treinamento. Essa propriedade se mostrou verdadeira em muitos casos, incluindo os *transformers*. O mesmo motivo leva √† aplica√ß√£o da *LayerNorm*, que nesse caso, normaliza os valores dos gradientes no processo de *backpropagation*.

A ideia de aplicar uma rede neural *feed-forward* em um bloco de *transformer* √© que durante o treino, essa rede aprenda durante o treinamento a converter cada elemento da sequ√™ncia em elementos da outra sequ√™ncia que estamos tentando gerar. Por√©m, n√£o entenda que isso significa que as duas sequ√™ncias v√£o ter sempre o mesmo tamanho, porque veremos que ainda √© poss√≠vel gerar sequ√™ncias de tamanho diferente dessa forma.

```python
@dataclass
class TransformerLayerConfig:
    embed_dim: int = 512
    n_heads: int = 8
    hidden_dim: int = 2048


class TransformerLayer(nn.Module):
    def __init__(self: Self, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.config = config

        self.mha = MultiheadAttention(
            embed_dim=self.config.embed_dim,
            n_heads=self.config.n_heads,
        )
        self.mha_layernorm = nn.LayerNorm(normalized_shape=self.config.embed_dim)

        self.ff = nn.Sequential(
            nn.Linear(self.config.embed_dim, self.config.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.config.hidden_dim, self.config.embed_dim),
        )
        self.ff_layernorm = nn.LayerNorm(self.config.embed_dim)

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        mha_res = values

        mha_outputs = self.mha(queries, keys, values, mask)
        mha_res_outputs = mha_outputs + mha_res
        norm_mha_outputs = self.mha_layernorm(mha_res_outputs)

        ff_res = norm_mha_outputs
        ff_outputs = self.ff(norm_mha_outputs)
        norm_ff_outputs = self.ff_layernorm(ff_outputs + ff_res)

        return norm_ff_outputs
```

### *Encoder*

```mermaid
flowchart
    input(Sequ√™ncia recebida)
    EncoderBlock(1¬∫ Bloco de *transformer*)
    EncoderBlock2(2¬∫ Bloco de *transformer*)
    EncoderBlockN(m¬∫ Bloco de *transformer*)
    outputN(Sequ√™ncia gerada pelo *encoder*)
    input --> EncoderBlock
    EncoderBlock --> EncoderBlock2
    EncoderBlock2 -- ... --> EncoderBlockN
    EncoderBlockN --> outputN
```

O codificador ou *encoder* √© respons√°vel por converter uma sequ√™ncia recebida em outra sequ√™ncia latente. Isso significa que esse valor existe em um espa√ßo que pode n√£o ter um significado claro fora da arquitetura, mas durante o treinamento, o *encoder* √© otimizado de forma que o resultado desse componente serve de suporte para que o modelo como um todo possa gerar melhores resultados. No caso, o objetivo do *encoder* √© criar uma representa√ß√£o da sequ√™ncia que ser√° usada de forma mais eficiente pelo *decoder* posteriormente como vari√°vel independente adicional.

Um *encoder* √© composto de uma s√©rie de blocos de *transformer* aplicados em s√©rie sobre a sequ√™ncia original: o 1¬∫ bloco usar√° a sequ√™ncia original como *Queries*, *Keys* e *Values*, o 2¬∫ bloco usar√° a sequ√™ncia do 1¬∫ bloco como *Queries*, *Keys* e *Values*, e assim por diante, at√© se obter a sequ√™ncia gerada pelo $m$¬∫ bloco, que ser√° considerada a sequ√™ncia gerada pelo *encoder*.

```python
class Encoder(nn.Module):
    def __init__(self: Self, n_layers: int, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.n_layers = n_layers
        self.config = config
        self.layers = nn.ModuleList(
            TransformerLayer(self.config) for _ in range(self.n_layers)
        )

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
    ) -> torch.Tensor:
        layer, *layers = self.layers
        layer_outputs = layer(
            queries=queries,
            keys=keys,
            values=values,
        )

        for layer in layers:
            layer_outputs = layer(
                queries=layer_outputs,
                keys=layer_outputs,
                values=layer_outputs,
            )
        return layer_outputs
```

### *Decoder*

O *decoder* √© respons√°vel por gerar, a partir sequ√™ncia recebida com *shift* e a sequ√™ncia gerada pelo *encoder*. Al√©m disso, tamb√©m √© composto de blocos iguais aplicados em s√©rie, mas os blocos usados s√£o um pouco diferentes:

```mermaid
flowchart
    encoderOutput(Sequ√™ncia gerada
pelo *encoder*)
    Value(*Values*)
    Key(*Keys*)
    Query(*Queries*)
    MHA(*Multihead Attention*)
    DecoderBlock(Bloco de *transformer*)
    decoderBlockOutput(Sequ√™ncia gerada)
    attentionMask(M√°scara de aten√ß√£o)
    attentionMask --> MHA
    Query --> MHA
    Key --> MHA
    Value --> MHA
    MHA -- Value --> DecoderBlock
    encoderOutput -- Query, Key --> DecoderBlock
    DecoderBlock --> decoderBlockOutput
```

Esses blocos possuem mais uma opera√ß√£o de *Multihead Attention* a partir da sequ√™ncia recebida, e nesse caso, ser√° usada a m√°scara de aten√ß√£o. A sequ√™ncia gerada nessa etapa √© usada como os *values* em um bloco de *transformer*, e as *queries* e *keys* ser√£o a sequ√™ncia gerada pelo *encoder*. Realizar essa mudan√ßa transforma o mecanismo em outro, denominado *Encoder-Decoder Attention*, e esse caso √© um dos motivos pelos quais damos tr√™s nomes diferentes para as vari√°veis de entrada mesmo que sejam iguais em outras etapas.

```python
class DecoderLayer(nn.Module):
    def __init__(self: Self, config: TransformerLayerConfig) -> None:
        super().__init__()
        self.config = config
        self.mha = MultiheadAttention(
            embed_dim=self.config.embed_dim,
            n_heads=self.config.n_heads,
        )
        self.transformer_layer = TransformerLayer(self.config)

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        encoder_outputs: torch.Tensor,
    ) -> torch.Tensor:
        batches, tokens, _ = keys.size()
        mask = get_attn_mask((batches, self.config.n_heads, tokens, tokens))
        outputs = self.mha(
            queries=queries,
            keys=keys,
            values=values,
            mask=mask,
        )
        outputs = self.transformer_layer(
            queries=encoder_outputs,
            keys=encoder_outputs,
            values=outputs,
        )
        return outputs
```

```mermaid
flowchart
    input("`Sequ√™ncia recebida (com *shift*)`")
    encoderOutput("`Sequ√™ncia gerada pelo *encoder*`")
    DecoderBlock1(1¬∫ bloco de *decoder*)
    DecoderBlock2(2¬∫ bloco de *decoder*)
    DecoderBlockN(n¬∫ bloco de *decoder*)
    output(Output)

    input -- *Queries, Keys, Values* --> DecoderBlock1
    encoderOutput --> DecoderBlock1

    DecoderBlock1 -- *Queries, Keys, Values* --> DecoderBlock2
    encoderOutput --> DecoderBlock2
    
    DecoderBlock2 -- ... --> DecoderBlockN
    encoderOutput --> DecoderBlockN
    
    DecoderBlockN --> output
```

O *decoder* √© composto de uma s√©rie de blocos de *decoder*, onde o primeiro bloco recebe a sequ√™ncia com *shift*, o segundo bloco recebe a sequ√™ncia gerada pelo primeiro bloco, e assim por diante, at√© se obter a sequ√™ncia gerada pelo $n$¬∫ bloco, que ser√° considerada a sequ√™ncia gerada pelo *decoder*. Em todos os casos, ser√° usada a mesma sequ√™ncia gerada pelo *encoder*.

Ao usar blocos que determinam a aten√ß√£o que deve ser prestada em cada elemento da sequ√™ncia recebida (ap√≥s a passagem pelo *encoder*), mas geram a sequ√™ncia usando os elementos da sequ√™ncia com *shift*, o *decoder* √© treinado para conseguir realizar *shift* na sequ√™ncia recebida sem saber qual √© a sequ√™ncia esperada. E como o primeiro *shift* sempre ser√° remover o *token* `<bos>` do in√≠cio da sequ√™ncia e adicion√°-lo ao final, sempre √© poss√≠vel realizar a primeira predi√ß√£o do modelo, cujo *token* gerado pode ser usado para gerar a segunda predi√ß√£o do modelo, e assim por diante. Dessa forma √© poss√≠vel gerar uma sequ√™ncia nova inteira usando apenas a sequ√™ncia recebida original.

```python
class Decoder(nn.Module):
    def __init__(
        self: Self,
        n_layers: int,
        config: TransformerLayerConfig,
    ) -> None:
        super().__init__()
        self.n_layers = n_layers
        self.config = config
        self.layers = nn.ModuleList(
            DecoderLayer(self.config) for _ in range(self.n_layers)
        )

    def forward(
        self: Self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        encoder_outputs: torch.Tensor,
    ) -> torch.Tensor:
        layer, *layers = self.layers
        layer_outputs = layer(
            queries=queries,
            keys=keys,
            values=values,
            encoder_outputs=encoder_outputs,
        )

        for layer in layers:
            layer_outputs = layer(
                queries=layer_outputs,
                keys=layer_outputs,
                values=layer_outputs,
                encoder_outputs=encoder_outputs,
            )
        return layer_outputs
```

### Processamento de sa√≠da

```mermaid
flowchart
    DecoderBlockN(Input)
    linear(Linear)
    Softmax(Softmax)
    Argmax(Argmax)
    outputString(Output)
    DecoderBlockN --> linear
    linear --> Softmax
    Softmax --> Argmax
    Argmax --> outputString
```

Por √∫ltimo, a sequ√™ncia gerada pelo decoder √© transformada em *tokens* do vocabul√°rio de sa√≠da (lembre-se que ele pode ser diferente do vocabul√°rio de entrada caso desejar). A forma com que isso √© feita consiste em:

* Aplicar uma transforma√ß√£o linear na sequ√™ncia gerada, que leva a dimens√£o dos elementos da sequ√™ncia de $d_{in}$ para $|v_{out}|$, o n√∫mero de *tokens* no vocabul√°rio de sa√≠da.
* Aplicar a fun√ß√£o *softmax* em cada elemento da sequ√™ncia, normalizando os valores dos elementos e fazendo com que representem as probabilidades de cada *token* ser o valor predito para cada posi√ß√£o da sequ√™ncia.
* Aplicar a func√£o *argmax* em cada elemento da sequ√™ncia, que obt√©m o √≠ndice com maior probabilidade de cada elemento e fazendo com que representem o *token* mais prov√°vel para cada posi√ß√£o da sequ√™ncia.

Nesse ponto, durante o treinamento, j√° √© poss√≠vel calcular a perda comparando os *tokens* esperados e os preditos na sua forma num√©rica. Para gerar um texto, por exemplo, ainda √© necess√°rio converter os *tokens* de volta para seus valores textuais (no nosso caso, caracteres) e concaten√°-los.

```python
class OutputProcessor(nn.Module):
    def __init__(self: Self, embed_dim: int, vocab: str) -> None:
        super().__init__()
        self.embed_dim = embed_dim
        self.vocab = set(vocab)
        self.out_dim = len(self.vocab)
        self.linear = nn.Linear(self.embed_dim, self.out_dim)
        self.id_chars = {index: char for index, char in enumerate(self.vocab)}

    def untokenize(self: Self, tokens: list[int]) -> str:
        return "".join(self.id_chars[token] for token in tokens)

    def forward(self: Self, inputs: torch.Tensor, token_counts: list[int]) -> list[str]:
        linear_outputs = self.linear(inputs)
        batches, tokens, _ = linear_outputs.size()

        probs = linear_outputs.softmax(dim=2)
        probs = probs.view(batches * tokens, self.out_dim)

        predictions = probs.argmax(dim=1)
        predictions = predictions.view(batches, tokens)

        outputs = [self.untokenize(tokens) for tokens in predictions.tolist()]
        outputs = [
            output[:token_count] for output, token_count in zip(outputs, token_counts)
        ]

        return outputs
```

E assim, est√° feita a implementa√ß√£o da arquitetura de um *transformer* de forma completa.

```python
class Transformer(nn.Module):
    def __init__(
        self: Self,
        encoder_config: TransformerLayerConfig,
        decoder_config: TransformerLayerConfig,
        in_vocab: str | set[str],
        out_vocab: str | set[str],
        embed_dim: int = 512,
        theta: int = 10000,
        n_encoder_layers: int = 6,
        n_decoder_layers: int = 6,
    ) -> None:
        super().__init__()

        self.theta = theta
        self.encoder_config = encoder_config
        self.n_encoder_layers = n_encoder_layers
        self.decoder_config = decoder_config
        self.n_decoder_layers = n_decoder_layers
        self.embed_dim = embed_dim

        self.in_vocab = set(in_vocab)
        self.out_vocab = set(out_vocab)

        self.tokenizer = Tokenizer(
            embed_dim=self.embed_dim,
            vocab=self.in_vocab,
        )

        self.positional_encoder = PositionalEncoder(
            embed_dim=self.embed_dim,
            theta=self.theta,
        )

        self.input_processor = InputProcessor(
            tokenizer=self.tokenizer,
            positional_encoder=self.positional_encoder,
        )

        self.encoder = Encoder(
            n_layers=self.n_encoder_layers,
            config=self.encoder_config,
        )

        self.decoder = Decoder(
            n_layers=self.n_decoder_layers,
            config=self.decoder_config,
        )

        self.output_processor = OutputProcessor(
            embed_dim=self.embed_dim,
            vocab=self.out_vocab,
        )

    def forward(self: Self, inputs: list[str]) -> str:
        input_tensors, token_counts = self.input_processor(inputs)

        encoder_outputs = self.encoder(
            queries=input_tensors,
            keys=input_tensors,
            values=input_tensors,
        )

        decoder_outputs = self.decoder(
            queries=input_tensors,
            keys=input_tensors,
            values=input_tensors,
            encoder_outputs=encoder_outputs,
        )

        outputs = self.output_processor(decoder_outputs, token_counts)
        return outputs
```

Um exemplo de execu√ß√£o:

```python
config = TransformerLayerConfig()

transformer = Transformer(
    in_vocab=printable,
    out_vocab=printable,
    encoder_config=config,
    decoder_config=config,
)

inputs = ["nom", "par", "ca", "da"]

def separate_chars(chars):
    return " ".join(repr(char) for char in chars)

lines = [
    f"{separate_chars(item)} -> {separate_chars(output)}"
    for item, output in zip(inputs, transformer(inputs))
]

print(*lines, sep="\n")
```
